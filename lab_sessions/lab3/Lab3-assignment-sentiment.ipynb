{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 - Assignment Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the LAB-2 assignment of the Text Mining course. It is about sentiment analysis.\n",
    "\n",
    "The aims of the assignment are:\n",
    "* Learn how to run a rule-based sentiment analysis module (VADER)\n",
    "* Learn how to run a machine learning sentiment analysis module (Scikit-Learn/ Naive Bayes)\n",
    "* Learn how to run scikit-learn metrics for the quantitative evaluation\n",
    "* Learn how to perform and interpret a quantitative evaluation of the outcomes of the tools (in terms of Precision, Recall, and F<sub>1</sub>)\n",
    "* Learn how to evaluate the results qualitatively (by examining the data) \n",
    "* Get insight into differences between the two applied methods\n",
    "* Get insight into the effects of using linguistic preprocessing\n",
    "* Be able to describe differences between the two methods in terms of their results\n",
    "* Get insight into issues when applying these methods across different  domains\n",
    "\n",
    "In this assignment, you are going to create your own gold standard set from 50 tweets. You will the VADER and scikit-learn classifiers to these tweets and evaluate the results by using evaluation metrics and inspecting the data.\n",
    "\n",
    "We recommend you go through the notebooks in the following order:\n",
    "* **Read the assignment (see below)**\n",
    "* **Lab3.2-Sentiment-analysis-with-VADER.ipynb**\n",
    "* **Lab3.3-Sentiment-analysis.with-scikit-learn.ipynb**\n",
    "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
    "\n",
    "In this assignment you are asked to perform both quantitative evaluations and error analyses:\n",
    "* a quantitative evaluation concerns the scores (Precision, Recall, and F<sub>1</sub>) provided by scikit's classification_report. It includes the scores per category, as well as micro and macro averages. Discuss whether the scores are balanced or not between the different categories (positive, negative, neutral) and between precision and recall. Discuss the shortcomings (if any) of the classifier based on these scores\n",
    "* an error analysis regarding the misclassifications of the classifier. It involves going through the texts and trying to understand what has gone wrong. It servers to get insight in what could be done to improve the performance of the classifier. Do you observe patterns in misclassifications?  Discuss why these errors are made and propose ways to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "The notebooks in this block have been originally created by [Marten Postma](https://martenpostma.github.io) and [Isa Maks](https://research.vu.nl/en/persons/e-maks). Adaptations were made by [Filip Ilievski](http://ilievski.nl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: VADER assignments\n",
    "\n",
    "\n",
    "### Preparation (nothing to submit):\n",
    "To be able to answer the VADER questions you need to know how the tool works. \n",
    "* Read more about the VADER tool in [this blog](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html).  \n",
    "* VADER provides 4 scores (positive, negative, neutral, compound). Be sure to understand what they mean and how they are calculated.\n",
    "* VADER uses rules to handle linguistic phenomena such as negation and intensification. Be sure to understand which rules are used, how they work, and why they are important.\n",
    "* VADER makes use of a sentiment lexicon. Have a look at the lexicon. Be sure to understand which information can be found there (lemma?, wordform?, part-of-speech?, polarity value?, word meaning?) What do all scores mean? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) \n",
    "\n",
    "\n",
    "### [3.5 points] Question1:\n",
    "\n",
    "Regard the following sentences and their output as given by VADER. Regard sentences 1 to 7, and explain the outcome **for each sentence**. Take into account both the rules applied by VADER and the lexicon that is used. You will find that some of the results are reasonable, but others are not. Explain what is going wrong or not when correct and incorrect results are produced. \n",
    "\n",
    "```\n",
    "INPUT SENTENCE 1 I love apples\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
    "\n",
    "INPUT SENTENCE 2 I don't love apples\n",
    "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
    "\n",
    "INPUT SENTENCE 3 I love apples :-)\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
    "\n",
    "INPUT SENTENCE 4 These houses are ruins\n",
    "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
    "\n",
    "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
    "\n",
    "INPUT SENTENCE 6 He lies in the chair in the garden\n",
    "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
    "\n",
    "INPUT SENTENCE 7 This house is like any house\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "VADER uses a lexicon which consits of word which are sentiment-related.\n",
    "\n",
    "Sentence 1: Here, VADER picks up the word 'love' and rates the sentence positively.\n",
    "\n",
    "Sentence 2: Additionally, VADER picks up the word 'don't' and together with 'love' assigns the sentence negatively.\n",
    "\n",
    "Sentence 3: VADER can also pick up emoticons which it does here and assigns it more positively than without the happy smiley face.\n",
    "\n",
    "Sentence 4: VADER picks up the word 'ruins' which has a negative connotations because of the verb 'ruin' while in context 'ruins' is just a state of buildings.\n",
    "\n",
    "Sentence 5: Here, VADER sees the word 'ruins' again but also the word 'not' and so combining them makes it something positive while it should mostly be neutral as it is just a statement. \n",
    "\n",
    "Sentence 6: Similar to sentence 4, VADER picks up the word 'lies' which has a negative connotation because of the verb 'lying' and thus assigns the sentence a bit negatively. This sentence should only be classified neutral.\n",
    "\n",
    "Sentence 7: Similar to the last sentence, it picks up the word 'like' which has a positive connotation so it assigns the sentence positively but in this context it is just to compare to the other house and should instead be more neutral. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Points: 2.5] Exercise 2: Collecting 50 tweets for evaluation\n",
    "Collect 50 tweets. Try to find tweets that are interesting for sentiment analysis, e.g., very positive, neutral, and negative tweets. These could be your own tweets (typed in) or collected from the Twitter stream.\n",
    "\n",
    "We will store the tweets in the file **my_tweets.json** (use a text editor to edit).\n",
    "For each tweet, you should insert:\n",
    "* sentiment analysis label: negative | neutral | positive (this you determine yourself, this is not done by a computer)\n",
    "* the text of the tweet\n",
    "* the Tweet-URL\n",
    "\n",
    "from:\n",
    "```\n",
    "    \"1\": {\n",
    "        \"sentiment_label\": \"\",\n",
    "        \"text_of_tweet\": \"\",\n",
    "        \"tweet_url\": \"\",\n",
    "```\n",
    "to:\n",
    "```\n",
    "\"1\": {\n",
    "        \"sentiment_label\": \"positive\",\n",
    "        \"text_of_tweet\": \"All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\",\n",
    "        \"tweet_url\" : \"https://twitter.com/BarackObama/status/946775615893655552\",\n",
    "    },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load your tweets with human annotation in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweets = json.load(open('my_tweets.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'sentiment_label': 'positive', 'text_of_tweet': 'The Northern Lights, an atmospheric phenomenon rarely seen in the Netherlands, were visible over large parts of the country on Sunday night.', 'tweet_url': 'https://twitter.com/DutchNewsNL/status/1630281109274599425'}\n"
     ]
    }
   ],
   "source": [
    "for id_, tweet_info in my_tweets.items():\n",
    "    print(id_, tweet_info)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Question 3:\n",
    "\n",
    "Run VADER on your own tweets (see function **run_vader** from notebook **Lab2-Sentiment-analysis-using-VADER.ipynb**). You can use the code snippet below this explanation as a starting point. \n",
    "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
    "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copied from Lab3.2\n",
    "from nltk.sentiment import vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "nlp = spacy.load('en_core_web_sm') # 'en_core_web_sm'\n",
    "\n",
    "def run_vader(textual_unit, \n",
    "              lemmatize=False,\n",
    "              parts_of_speech_to_consider=set(),\n",
    "              verbose=1):\n",
    "   \n",
    "    doc = nlp(textual_unit)\n",
    "        \n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == '-PRON-': \n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add) \n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print()\n",
    "        print('INPUT SENTENCE', sent)\n",
    "        print('INPUT TO VADER', input_to_vader)\n",
    "        print('VADER OUTPUT', scores)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INPUT SENTENCE The Northern Lights, an atmospheric phenomenon rarely seen in the Netherlands, were visible over large parts of the country on Sunday night.\n",
      "INPUT TO VADER ['the', 'Northern', 'Lights', ',', 'an', 'atmospheric', 'phenomenon', 'rarely', 'see', 'in', 'the', 'Netherlands', ',', 'be', 'visible', 'over', 'large', 'part', 'of', 'the', 'country', 'on', 'Sunday', 'night', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Tweet: 1 The Northern Lights, an atmospheric phenomenon rarely seen in the Netherlands, were visible over large parts of the country on Sunday night.\n",
      "Vader label: neutral\n",
      "Gold label: positive\n",
      "\n",
      "\n",
      "INPUT SENTENCE actually cannot breathe from how wholesome this is, not a single bad thing happens in it & I was real scared for a minute there\n",
      "INPUT TO VADER ['actually', 'can', 'not', 'breathe', 'from', 'how', 'wholesome', 'this', 'be', ',', 'not', 'a', 'single', 'bad', 'thing', 'happen', 'in', 'it', '&', 'I', 'be', 'real', 'scared', 'for', 'a', 'minute', 'there']\n",
      "VADER OUTPUT {'neg': 0.113, 'neu': 0.777, 'pos': 0.111, 'compound': -0.0129}\n",
      "Tweet: 2 actually cannot breathe from how wholesome this is, not a single bad thing happens in it & I was real scared for a minute there\n",
      "Vader label: negative\n",
      "Gold label: positive\n",
      "\n",
      "\n",
      "INPUT SENTENCE there are like 4 ppl that hacked into my netflix from different countries so I just left them a message to try and organize a movie night with all of them\n",
      "INPUT TO VADER ['there', 'be', 'like', '4', 'ppl', 'that', 'hack', 'into', 'my', 'netflix', 'from', 'different', 'country', 'so', 'I', 'just', 'leave', 'they', 'a', 'message', 'to', 'try', 'and', 'organize', 'a', 'movie', 'night', 'with', 'all', 'of', 'they']\n",
      "VADER OUTPUT {'neg': 0.051, 'neu': 0.863, 'pos': 0.086, 'compound': 0.2551}\n",
      "Tweet: 3 there are like 4 ppl that hacked into my netflix from different countries so I just left them a message to try and organize a movie night with all of them\n",
      "Vader label: positive\n",
      "Gold label: neutral\n",
      "\n",
      "\n",
      "INPUT SENTENCE by consuming it into the body, it extracts the very essence of the soul, allowing the body to be more susceptible to LOW frequencies & even demonic energies\n",
      "INPUT TO VADER ['Kali', 'Uchis', 'condemn', 'the', 'use', 'of', 'alcohol', 'in', 'new', 'Instagram', 'story', ':', 'please', 'stop', 'drink', 'liquor', '!', '!', '!', 'by', 'consume', 'it', 'into', 'the', 'body', ',', 'it', 'extract', 'the', 'very', 'essence', 'of', 'the', 'soul', ',', 'allow', 'the', 'body', 'to', 'be', 'more', 'susceptible', 'to', 'low', 'frequency', '&', 'even', 'demonic', 'energy']\n",
      "VADER OUTPUT {'neg': 0.16, 'neu': 0.715, 'pos': 0.125, 'compound': -0.4097}\n",
      "Tweet: 4 Kali Uchis condemns the use of alcohol in new Instagram story: Please stop drinking liquor !!! by consuming it into the body, it extracts the very essence of the soul, allowing the body to be more susceptible to LOW frequencies & even demonic energies\n",
      "Vader label: negative\n",
      "Gold label: negative\n",
      "\n",
      "\n",
      "INPUT SENTENCE flower room\n",
      "INPUT TO VADER ['flower', 'room']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Tweet: 5 flower room\n",
      "Vader label: neutral\n",
      "Gold label: neutral\n",
      "\n",
      "\n",
      "INPUT SENTENCE i would have wanted nobody other than ke huy quan photobombing andrew and zendaya this is everything to me omg\n",
      "INPUT TO VADER ['I', 'would', 'have', 'want', 'nobody', 'other', 'than', 'ke', 'huy', 'quan', 'photobombe', 'andrew', 'and', 'zendaya', 'this', 'be', 'everything', 'to', 'I', 'omg']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.929, 'pos': 0.071, 'compound': 0.0772}\n",
      "Tweet: 6 i would have wanted nobody other than ke huy quan photobombing andrew and zendaya this is everything to me omg\n",
      "Vader label: positive\n",
      "Gold label: positive\n",
      "\n",
      "\n",
      "INPUT SENTENCE Baby Lion Tamarin monkey Rescued from the road and Returned to Mother\n",
      "INPUT TO VADER ['Baby', 'Lion', 'Tamarin', 'monkey', 'rescue', 'from', 'the', 'road', 'and', 'return', 'to', 'Mother']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.769, 'pos': 0.231, 'compound': 0.5106}\n",
      "Tweet: 7 Baby Lion Tamarin monkey Rescued from the road and Returned to Mother\n",
      "Vader label: positive\n",
      "Gold label: positive\n",
      "\n",
      "\n",
      "INPUT SENTENCE PEOPLE CAN'T REALLY THINK LIKE THIS\n",
      "INPUT TO VADER ['IKYFL', ':(', ':(', ':(', 'PEOPLE', \"can't\", 'REALLY', 'think', 'like', 'this']\n",
      "VADER OUTPUT {'neg': 0.658, 'neu': 0.342, 'pos': 0.0, 'compound': -0.8893}\n",
      "Tweet: 8 IKYFL :( :( :( PEOPLE CAN'T REALLY THINK LIKE THIS\n",
      "Vader label: negative\n",
      "Gold label: negative\n",
      "\n",
      "\n",
      "INPUT SENTENCE Betty Boothroyd, first woman to become Commons Speaker in UK parliament, has died aged 93\n",
      "INPUT TO VADER ['Betty', 'Boothroyd', ',', 'first', 'woman', 'to', 'become', 'Commons', 'Speaker', 'in', 'UK', 'parliament', ',', 'have', 'die', 'aged', '93']\n",
      "VADER OUTPUT {'neg': 0.218, 'neu': 0.782, 'pos': 0.0, 'compound': -0.5994}\n",
      "Tweet: 9 Betty Boothroyd, first woman to become Commons Speaker in UK parliament, has died aged 93\n",
      "Vader label: negative\n",
      "Gold label: negative\n",
      "\n",
      "\n",
      "INPUT SENTENCE i‚Äôm crying\n",
      "INPUT TO VADER ['I', '‚Äôm', 'cry']\n",
      "VADER OUTPUT {'neg': 0.756, 'neu': 0.244, 'pos': 0.0, 'compound': -0.4767}\n",
      "Tweet: 10 i‚Äôm crying\n",
      "Vader label: negative\n",
      "Gold label: negative\n",
      "\n",
      "\n",
      "INPUT SENTENCE #NAACPImageAwards #SAGAwards\n",
      "INPUT TO VADER ['we', 'have', 'a', 'great', 'weekend', '.', 'so', 'grateful', '.', '#', 'naacpimageaward', '#', 'sagaward']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.448, 'pos': 0.552, 'compound': 0.8122}\n",
      "Tweet: 11 We had a great weekend. So grateful. #NAACPImageAwards #SAGAwards\n",
      "Vader label: positive\n",
      "Gold label: positive\n",
      "\n",
      "\n",
      "INPUT SENTENCE How luggage is loaded on airplanes.\n",
      "INPUT TO VADER ['how', 'luggage', 'be', 'load', 'on', 'airplane', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Tweet: 12 How luggage is loaded on airplanes.\n",
      "Vader label: neutral\n",
      "Gold label: neutral\n",
      "\n",
      "\n",
      "INPUT SENTENCE Batman and Robin meet Sammy Davis Jr. (1966)\n",
      "INPUT TO VADER ['Batman', 'and', 'Robin', 'meet', 'Sammy', 'Davis', 'Jr.', '(', '1966', ')']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Tweet: 13 Batman and Robin meet Sammy Davis Jr. (1966)\n",
      "Vader label: neutral\n",
      "Gold label: neutral\n",
      "\n",
      "\n",
      "INPUT SENTENCE sleep deprivation twitter wya :(\n",
      "INPUT TO VADER ['sleep', 'deprivation', 'twitter', 'wya', ':(']\n",
      "VADER OUTPUT {'neg': 0.655, 'neu': 0.345, 'pos': 0.0, 'compound': -0.6908}\n",
      "Tweet: 14 sleep deprivation twitter wya :(\n",
      "Vader label: negative\n",
      "Gold label: negative\n",
      "\n",
      "\n",
      "INPUT SENTENCE Just a bush.\n",
      "INPUT TO VADER ['just', 'a', 'bush', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Tweet: 15 Just a bush.\n",
      "Vader label: neutral\n",
      "Gold label: neutral\n",
      "\n",
      "\n",
      "INPUT SENTENCE A huge portion of those sent by russia to die in Ukraine are ethnic minorities from its colonies\n",
      "INPUT TO VADER ['a', 'huge', 'portion', 'of', 'those', 'send', 'by', 'russia', 'to', 'die', 'in', 'Ukraine', 'be', 'ethnic', 'minority', 'from', 'its', 'colony']\n",
      "VADER OUTPUT {'neg': 0.184, 'neu': 0.708, 'pos': 0.108, 'compound': -0.3818}\n",
      "Tweet: 16 A huge portion of those sent by russia to die in Ukraine are ethnic minorities from its colonies\n",
      "Vader label: negative\n",
      "Gold label: negative\n",
      "\n",
      "\n",
      "INPUT SENTENCE the photographer called him pretty\n",
      "INPUT TO VADER ['the', 'photographer', 'call', 'he', 'pretty']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.556, 'pos': 0.444, 'compound': 0.4939}\n",
      "Tweet: 17 the photographer called him pretty\n",
      "Vader label: positive\n",
      "Gold label: positive\n",
      "\n",
      "\n",
      "INPUT SENTENCE | @Reuters\n",
      "INPUT TO VADER ['thousand', 'take', 'to', 'Lisbon', '‚Äôs', 'street', 'on', 'Saturday', 'to', 'demand', 'well', 'living', 'condition', 'at', 'a', 'time', 'high', 'inflation', 'be', 'make', 'it', 'even', 'tough', 'for', 'people', 'to', 'make', 'end', 'meet', '.', '|', '@reuter']\n",
      "VADER OUTPUT {'neg': 0.096, 'neu': 0.836, 'pos': 0.068, 'compound': 0.0258}\n",
      "Tweet: 18 Thousands took to Lisbon‚Äôs streets on Saturday to demand better living conditions at a time high inflation is making it even tougher for people to make ends meet. | @Reuters\n",
      "Vader label: positive\n",
      "Gold label: neutral\n",
      "\n",
      "\n",
      "INPUT SENTENCE An alignment of Earth, Mars, Venus, Saturn, and Jupiter\n",
      "INPUT TO VADER ['an', 'alignment', 'of', 'Earth', ',', 'Mars', ',', 'Venus', ',', 'Saturn', ',', 'and', 'Jupiter']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Tweet: 19 An alignment of Earth, Mars, Venus, Saturn, and Jupiter\n",
      "Vader label: neutral\n",
      "Gold label: neutral\n",
      "\n",
      "\n",
      "INPUT SENTENCE Scientists explain everything by a recent solar flare.\n",
      "INPUT TO VADER ['last', 'night', ',', 'the', 'northern', 'light', 'be', 'observe', 'by', 'resident', 'of', 'those', 'country', 'where', 'it', 'be', 'almost', 'never', 'see', '.', ' ', 'an', 'unusual', 'phenomenon', 'be', 'see', 'by', 'resident', 'of', 'Britain', ',', 'Denmark', ',', 'the', 'Netherlands', 'and', 'the', 'United', 'States', '.', 'scientist', 'explain', 'everything', 'by', 'a', 'recent', 'solar', 'flare', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.935, 'pos': 0.065, 'compound': 0.4215}\n",
      "Tweet: 20 Last night, the northern lights were observed by residents of those countries where it is almost never seen.  An unusual phenomenon was seen by residents of Britain, Denmark, the Netherlands and the United States.Scientists explain everything by a recent solar flare.\n",
      "Vader label: positive\n",
      "Gold label: neutral\n",
      "\n",
      "\n",
      "INPUT SENTENCE Hong Kong‚Äôs leader said on Tuesday that the city would lift its mask mandate, one of the last such policies in the world, as it continues to wind down its once-stringent Covid control measures.\n",
      "INPUT TO VADER ['Hong', 'Kong', '‚Äôs', 'leader', 'say', 'on', 'Tuesday', 'that', 'the', 'city', 'would', 'lift', 'its', 'mask', 'mandate', ',', 'one', 'of', 'the', 'last', 'such', 'policy', 'in', 'the', 'world', ',', 'as', 'it', 'continue', 'to', 'wind', 'down', 'its', 'once', '-', 'stringent', 'covid', 'control', 'measure', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Tweet: 21 Hong Kong‚Äôs leader said on Tuesday that the city would lift its mask mandate, one of the last such policies in the world, as it continues to wind down its once-stringent Covid control measures.\n",
      "Vader label: neutral\n",
      "Gold label: neutral\n",
      "\n",
      "\n",
      "INPUT SENTENCE two faced POS üôÑ\n",
      "INPUT TO VADER ['and', 'she', 'can', 'demean', 'the', 'President', 'of', 'the', 'United', 'States', 'at', 'the', 'SOTU', 'speech', '....', 'two', 'face', 'POS', 'üôÑ']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.859, 'pos': 0.141, 'compound': 0.4215}\n",
      "Tweet: 22 And she can demean the President of the United States at the SOTU speech....two faced POS üôÑ\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "\n",
      "\n",
      "INPUT SENTENCE Honestly, Windows 11 is beautiful.\n",
      "INPUT TO VADER ['honestly', ',', 'Windows', '11', 'be', 'beautiful', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.303, 'pos': 0.697, 'compound': 0.7845}\n",
      "Tweet: 23 Honestly, Windows 11 is beautiful.\n",
      "Vader label: positive\n",
      "Gold label: positive\n",
      "\n",
      "\n",
      "INPUT SENTENCE You are still learning.\n",
      "INPUT TO VADER ['be', 'nice', 'to', 'yourself', '.', 'you', 'be', 'still', 'learn', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.714, 'pos': 0.286, 'compound': 0.4215}\n",
      "Tweet: 24 Be nicer to yourself. You are still learning.\n",
      "Vader label: positive\n",
      "Gold label: positive\n",
      "\n",
      "\n",
      "INPUT SENTENCE #100DaysOfCode #Java #gamedevelopment\n",
      "INPUT TO VADER ['day', '1', ':', 'just', 'write', 'my', 'first', 'little', 'game', 'use', 'Java', 'and', 'IT', 'works', '!', 'üéâ', 'you', 'try', 'to', 'catch', 'raindrop', 'use', 'the', 'bucket', '.', 'I', 'follow', 'a', 'tutorial', ',', 'but', 'I', 'be', 'still', 'so', 'excited', 'to', 'see', 'that', 'it', 'work', '\\U0001f979', '#', '100daysofcode', '#', 'Java', '#', 'gamedevelopment']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.884, 'pos': 0.116, 'compound': 0.6669}\n",
      "Tweet: 25 Day 1: Just wrote my first little game using Java and IT WORKS! üéâ You try to catch raindrops using the bucket. I followed a tutorial, but I'm still so excited to see that it works ü•π #100DaysOfCode #Java #gamedevelopment\n",
      "Vader label: positive\n",
      "Gold label: positive\n",
      "\n",
      "\n",
      "INPUT SENTENCE you blame us?\n",
      "INPUT TO VADER ['you', 'rob', 'we', 'of', 'our', 'natural', 'resource', ',', 'you', 'take', 'away', 'our', 'sustainability', ',', 'you', 'destroy', 'our', 'environment', '.', 'now', 'that', 'we', 'be', 'leave', 'to', 'use', 'plastic', 'as', 'the', 'cheap', 'alternative', 'because', 'we', 'be', 'poor', '...', 'you', 'blame', 'we', '?']\n",
      "VADER OUTPUT {'neg': 0.298, 'neu': 0.648, 'pos': 0.054, 'compound': -0.8834}\n",
      "Tweet: 26 You rob us of our natural resources, you take away our sustainability, you destroy our environment. Now that we are left to use plastics as the cheapest alternative because we are poor... you blame us?\n",
      "Vader label: negative\n",
      "Gold label: negative\n",
      "\n",
      "\n",
      "INPUT SENTENCE Continued prayers please.\n",
      "INPUT TO VADER ['so', 'I', 'test', 'positive', 'for', 'COVID', '.', 'continue', 'prayer', 'please', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.493, 'pos': 0.507, 'compound': 0.7334}\n",
      "Tweet: 27 So i tested positive for COVID. Continued prayers please.\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "\n",
      "\n",
      "INPUT SENTENCE Yoh\n",
      "INPUT TO VADER ['my', 'heart', 'get', 'so', 'full', 'everytime', 'I', 'see', 'someone', 'graduate', ',', 'because', 'nothing', 'about', 'varsity', 'be', 'easy', '!', 'that', 'thing', 'be', 'so', 'tough', '&', 'to', 'finally', 'reach', 'the', 'end', '?', 'yoh']\n",
      "VADER OUTPUT {'neg': 0.068, 'neu': 0.785, 'pos': 0.147, 'compound': 0.3184}\n",
      "Tweet: 28 My heart gets so full everytime I see someone graduate, because nothing about varsity is easy! That thing is so tough & to finally reach the end? Yoh\n",
      "Vader label: positive\n",
      "Gold label: positive\n",
      "\n",
      "\n",
      "INPUT SENTENCE ‚ÄúMA√ëANA SER√Å BONITO‚Äù by @KAROLG scores the biggest debut for a Spanish-language album by a woman in Spotify history.\n",
      "INPUT TO VADER ['\"', 'ma√±ana', 'SER√Å', 'bonito', '\"', 'by', '@KAROLG', 'score', 'the', 'big', 'debut', 'for', 'a', 'spanish', '-', 'language', 'album', 'by', 'a', 'woman', 'in', 'spotify', 'history', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Tweet: 29 ‚ÄúMA√ëANA SER√Å BONITO‚Äù by @KAROLG scores the biggest debut for a Spanish-language album by a woman in Spotify history.\n",
      "Vader label: neutral\n",
      "Gold label: neutral\n",
      "\n",
      "\n",
      "INPUT SENTENCE New Balance have won me over with these üòç\n",
      "INPUT TO VADER ['New', 'Balance', 'have', 'win', 'I', 'over', 'with', 'these', 'üòç']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.612, 'pos': 0.388, 'compound': 0.5859}\n",
      "Tweet: 30 New Balance have won me over with these üòç\n",
      "Vader label: positive\n",
      "Gold label: positive\n",
      "\n",
      "\n",
      "INPUT SENTENCE After a Marilyn Manson accuser claimed Evan Rachel Wood \"manipulated\" her, the actress provided a voicemail from the accuser saying she believed the rocker's attorney wanted the accuser to ‚Äúturn on the other girls and say that it was all a ruse‚Äù\n",
      "INPUT TO VADER ['after', 'a', 'Marilyn', 'Manson', 'accuser', 'claim', 'Evan', 'Rachel', 'Wood', '\"', 'manipulate', '\"', 'she', ',', 'the', 'actress', 'provide', 'a', 'voicemail', 'from', 'the', 'accuser', 'say', 'she', 'believe', 'the', 'rocker', \"'s\", 'attorney', 'want', 'the', 'accuser', 'to', '\"', 'turn', 'on', 'the', 'other', 'girl', 'and', 'say', 'that', 'it', 'be', 'all', 'a', 'ruse', '\"']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.968, 'pos': 0.032, 'compound': 0.0772}\n",
      "Tweet: 31 After a Marilyn Manson accuser claimed Evan Rachel Wood \"manipulated\" her, the actress provided a voicemail from the accuser saying she believed the rocker's attorney wanted the accuser to ‚Äúturn on the other girls and say that it was all a ruse‚Äù\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "\n",
      "\n",
      "INPUT SENTENCE European nations are struggling to deliver on promises to provide German-made tanks to Ukraine, exposing how unprepared and chronically underfunded their militaries are.\n",
      "INPUT TO VADER ['european', 'nation', 'be', 'struggle', 'to', 'deliver', 'on', 'promise', 'to', 'provide', 'German', '-', 'make', 'tank', 'to', 'Ukraine', ',', 'expose', 'how', 'unprepared', 'and', 'chronically', 'underfunded', 'their', 'military', 'be', '.']\n",
      "VADER OUTPUT {'neg': 0.143, 'neu': 0.772, 'pos': 0.085, 'compound': -0.1531}\n",
      "Tweet: 32 European nations are struggling to deliver on promises to provide German-made tanks to Ukraine, exposing how unprepared and chronically underfunded their militaries are.\n",
      "Vader label: negative\n",
      "Gold label: negative\n",
      "\n",
      "\n",
      "INPUT SENTENCE This is how my week goes: moooooooooooonday tuuuuuuuuesday weeeeeednesday thuuuursday friday weekend\n",
      "INPUT TO VADER ['this', 'be', 'how', 'my', 'week', 'go', ':', 'moooooooooooonday', 'tuuuuuuuuesday', 'weeeeeednesday', 'thuuuursday', 'friday', 'weekend']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Tweet: 33 This is how my week goes: moooooooooooonday tuuuuuuuuesday weeeeeednesday thuuuursday friday weekend\n",
      "Vader label: neutral\n",
      "Gold label: neutral\n",
      "\n",
      "\n",
      "INPUT SENTENCE I love your tweets and you are really funny.\n",
      "INPUT TO VADER ['I', 'love', 'your', 'tweet', 'and', 'you', 'be', 'really', 'funny', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.448, 'pos': 0.552, 'compound': 0.8122}\n",
      "Tweet: 34 I love your tweets and you are really funny.\n",
      "Vader label: positive\n",
      "Gold label: positive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INPUT SENTENCE Myspace Tom is a SAVAGE\n",
      "INPUT TO VADER ['Myspace', 'Tom', 'be', 'a', 'savage']\n",
      "VADER OUTPUT {'neg': 0.5, 'neu': 0.5, 'pos': 0.0, 'compound': -0.4588}\n",
      "Tweet: 35 Myspace Tom is a SAVAGE\n",
      "Vader label: negative\n",
      "Gold label: positive\n",
      "\n",
      "\n",
      "INPUT SENTENCE To Secede From Florida\n",
      "INPUT TO VADER ['Disney', 'World', 'Fortifies', 'Borders', 'with', 'Armed', 'Characters', 'as', 'Park', 'Announces', 'Plan', 'to', 'secede', 'from', 'Florida']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Tweet: 36 Disney World Fortifies Borders With Armed Characters As Park Announces Plan To Secede From Florida\n",
      "Vader label: neutral\n",
      "Gold label: negative\n",
      "\n",
      "\n",
      "INPUT SENTENCE What is carbon capture and how does it fight climate change?\n",
      "INPUT TO VADER ['what', 'be', 'carbon', 'capture', 'and', 'how', 'do', 'it', 'fight', 'climate', 'change', '?']\n",
      "VADER OUTPUT {'neg': 0.206, 'neu': 0.794, 'pos': 0.0, 'compound': -0.3818}\n",
      "Tweet: 37 What is carbon capture and how does it fight climate change?\n",
      "Vader label: negative\n",
      "Gold label: neutral\n",
      "\n",
      "\n",
      "INPUT SENTENCE Israeli-American killed in West Bank as unrest intensifies\n",
      "INPUT TO VADER ['israeli', '-', 'American', 'kill', 'in', 'West', 'Bank', 'as', 'unrest', 'intensifie']\n",
      "VADER OUTPUT {'neg': 0.37, 'neu': 0.63, 'pos': 0.0, 'compound': -0.6908}\n",
      "Tweet: 38 Israeli-American killed in West Bank as unrest intensifies\n",
      "Vader label: negative\n",
      "Gold label: negative\n",
      "\n",
      "\n",
      "INPUT SENTENCE TOMORROW X TOGETHER is now tied SEVENTEEN as the 3rd Korean Act with Most Cumulative weeks on Billboard Top Current Album Sales  (113 weeks each) üéâ\n",
      "INPUT TO VADER ['tomorrow', 'X', 'together', 'be', 'now', 'tie', 'SEVENTEEN', 'as', 'the', '3rd', 'Korean', 'Act', 'with', 'Most', 'cumulative', 'week', 'on', 'Billboard', 'Top', 'Current', 'Album', 'Sales', ' ', '(', '113', 'week', 'each', ')', 'üéâ']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.927, 'pos': 0.073, 'compound': 0.2023}\n",
      "Tweet: 39 TOMORROW X TOGETHER is now tied SEVENTEEN as the 3rd Korean Act with Most Cumulative weeks on Billboard Top Current Album Sales  (113 weeks each) üéâ\n",
      "Vader label: positive\n",
      "Gold label: positive\n",
      "\n",
      "\n",
      "INPUT SENTENCE WTF is this?Zelensky?#YouAreWatchingAMovie\n",
      "INPUT TO VADER ['WTF', 'be', 'this?Zelensky?#YouAreWatchingAMovie']\n",
      "VADER OUTPUT {'neg': 0.71, 'neu': 0.29, 'pos': 0.0, 'compound': -0.7089}\n",
      "Tweet: 40 WTF is this?Zelensky?#YouAreWatchingAMovie\n",
      "Vader label: negative\n",
      "Gold label: negative\n",
      "\n",
      "\n",
      "INPUT SENTENCE ‚ö°Ô∏è Photo from the crash site of the UJ-22 Airborne attack drone one hundred meters from the Voskresensk gas compressor station in the Kolomenskyi district of the moscow region, published by russian media.\n",
      "INPUT TO VADER ['‚ö°', 'Ô∏è', 'Photo', 'from', 'the', 'crash', 'site', 'of', 'the', 'UJ-22', 'Airborne', 'attack', 'drone', 'one', 'hundred', 'meter', 'from', 'the', 'Voskresensk', 'gas', 'compressor', 'station', 'in', 'the', 'Kolomenskyi', 'district', 'of', 'the', 'moscow', 'region', ',', 'publish', 'by', 'russian', 'medium', '.']\n",
      "VADER OUTPUT {'neg': 0.162, 'neu': 0.838, 'pos': 0.0, 'compound': -0.7003}\n",
      "Tweet: 41 ‚ö°Ô∏è Photo from the crash site of the UJ-22 Airborne attack drone one hundred meters from the Voskresensk gas compressor station in the Kolomenskyi district of the moscow region, published by russian media.\n",
      "Vader label: negative\n",
      "Gold label: negative\n",
      "\n",
      "\n",
      "INPUT SENTENCE The new CEO of Twitter is amazing\n",
      "INPUT TO VADER ['the', 'new', 'ceo', 'of', 'Twitter', 'be', 'amazing']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.612, 'pos': 0.388, 'compound': 0.5859}\n",
      "Tweet: 42 The new CEO of Twitter is amazing\n",
      "Vader label: positive\n",
      "Gold label: positive\n",
      "\n",
      "\n",
      "INPUT SENTENCE 1/2\n",
      "INPUT TO VADER ['I', '‚Äôm', 'so', 'pleased', 'to', 'share', 'that', 'the', 'Climate', 'Book', 'be', 'now', 'available', 'in', 'the', 'USA', 'and', 'Canada', '!', 'I', 'have', 'gather', 'the', 'wisdom', 'of', 'over', 'one', 'hundred', 'contributor', 'to', 'highlight', 'the', 'many', 'different', 'crisis', 'we', 'face', 'and', 'equip', 'we', 'with', 'the', 'knowledge', 'we', 'need', 'to', 'avoid', 'a', 'climate', 'disaster', '.', '1/2']\n",
      "VADER OUTPUT {'neg': 0.167, 'neu': 0.644, 'pos': 0.189, 'compound': 0.0897}\n",
      "Tweet: 43 I‚Äôm so pleased to share that The Climate Book is now available in the USA and Canada! I have gathered the wisdom of over one hundred contributors to highlight the many different crises we face and equip us with the knowledge we need to avoid a climate disaster. 1/2\n",
      "Vader label: positive\n",
      "Gold label: positive\n",
      "\n",
      "\n",
      "INPUT SENTENCE Please RT, educate your friends and family and demand action https://youtu.be/EOctIuyVfnA\n",
      "INPUT TO VADER ['Porto', ',', 'Portugal', '.', 'the', 'world', 'climate', 'be', 'destabilising', '.', 'scientist', 'be', 'scream', 'at', 'we', 'that', 'it', 'threaten', 'our', 'ability', 'to', 'grow', 'enough', 'food', 'and', 'we', 'may', 'permanently', 'collapse', 'human', 'civilisation', '.', 'please', 'RT', ',', 'educate', 'your', 'friend', 'and', 'family', 'and', 'demand', 'action', 'https://youtu.be/EOctIuyVfnA']\n",
      "VADER OUTPUT {'neg': 0.201, 'neu': 0.643, 'pos': 0.157, 'compound': -0.296}\n",
      "Tweet: 44 Porto, Portugal. The worlds climate is destabilising. Scientists are screaming at us that it threatens our ability to grow enough food and we may permanently collapse human civilisation. Please RT, educate your friends and family and demand action https://youtu.be/EOctIuyVfnA\n",
      "Vader label: negative\n",
      "Gold label: negative\n",
      "\n",
      "\n",
      "INPUT SENTENCE üíî #TurkeyEarthquake #TurkeyQuake\n",
      "INPUT TO VADER ['so', 'emotional', 'üòî', 'in', 'Hatay', ',', 'Turkey', ':', 'balloon', 'üéà', 'be', 'place', 'on', 'top', 'of', 'the', 'ruin', 'of', 'building', 'in', 'memory', 'of', 'the', 'child', 'who', 'perish', 'in', 'the', 'earthquake', '.', 'üíî', '#', 'TurkeyEarthquake', '#', 'TurkeyQuake']\n",
      "VADER OUTPUT {'neg': 0.121, 'neu': 0.762, 'pos': 0.117, 'compound': -0.2748}\n",
      "Tweet: 45 So emotional üòî In Hatay, Turkey: Balloons üéàwere placed on top of the ruins of buildings in memory of the children who perished in the earthquakes. üíî #TurkeyEarthquake #TurkeyQuake\n",
      "Vader label: negative\n",
      "Gold label: negative\n",
      "\n",
      "\n",
      "INPUT SENTENCE this is way too real üò≠ and then i get angry at him for being the most perfect person ever and loving him so much\n",
      "INPUT TO VADER ['this', 'be', 'way', 'too', 'real', 'üò≠', 'and', 'then', 'I', 'get', 'angry', 'at', 'he', 'for', 'be', 'the', 'most', 'perfect', 'person', 'ever', 'and', 'love', 'he', 'so', 'much']\n",
      "VADER OUTPUT {'neg': 0.105, 'neu': 0.635, 'pos': 0.26, 'compound': 0.7089}\n",
      "Tweet: 46 this is way too real üò≠ and then i get angry at him for being the most perfect person ever and loving him so much\n",
      "Vader label: positive\n",
      "Gold label: positive\n",
      "\n",
      "\n",
      "INPUT SENTENCE italy is such a beautiful country.\n",
      "INPUT TO VADER ['italy', 'be', 'such', 'a', 'beautiful', 'country', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.506, 'pos': 0.494, 'compound': 0.5994}\n",
      "Tweet: 47 italy is such a beautiful country.\n",
      "Vader label: positive\n",
      "Gold label: positive\n",
      "\n",
      "\n",
      "INPUT SENTENCE safety is our number one priority\n",
      "INPUT TO VADER ['safety', 'be', 'our', 'number', 'one', 'priority']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.494, 'pos': 0.506, 'compound': 0.4767}\n",
      "Tweet: 48 safety is our number one priority\n",
      "Vader label: positive\n",
      "Gold label: neutral\n",
      "\n",
      "\n",
      "INPUT SENTENCE Foreign Minister Kuleba and Infrastructure Minister Kubrakov say over 140 ships are waiting for checks, some in a queue for over a month.\n",
      "INPUT TO VADER ['Ukraine', 'have', 'accuse', 'Russia', 'of', 'sabotage', 'the', 'grain', 'corridor', ',', 'set', 'to', 'resolve', 'the', 'global', 'food', 'crisis', ',', 'by', 'deliberately', 'slow', 'down', 'inspection', '.', 'Foreign', 'Minister', 'Kuleba', 'and', 'Infrastructure', 'Minister', 'Kubrakov', 'say', 'over', '140', 'ship', 'be', 'wait', 'for', 'check', ',', 'some', 'in', 'a', 'queue', 'for', 'over', 'a', 'month', '.']\n",
      "VADER OUTPUT {'neg': 0.186, 'neu': 0.762, 'pos': 0.052, 'compound': -0.7717}\n",
      "Tweet: 49 Ukraine has accused Russia of sabotaging the grain corridor, set to resolve the global food crisis, by deliberately slowing down inspections. Foreign Minister Kuleba and Infrastructure Minister Kubrakov say over 140 ships are waiting for checks, some in a queue for over a month.\n",
      "Vader label: negative\n",
      "Gold label: negative\n",
      "\n",
      "\n",
      "INPUT SENTENCE Give Ukraine the ability to free the Black Sea Corridor.\n",
      "INPUT TO VADER ['russian', 'boat', 'need', 'to', 'be', 'remove', 'from', 'the', 'Black', 'Sea', '.', ' ', 'give', 'Ukraine', 'the', 'ability', 'to', 'free', 'the', 'Black', 'Sea', 'Corridor', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.763, 'pos': 0.237, 'compound': 0.6808}\n",
      "Tweet: 50 russian boats need to be removed from the Black Sea.  Give Ukraine the ability to free the Black Sea Corridor.\n",
      "Vader label: positive\n",
      "Gold label: neutral\n",
      "\n",
      "a:               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.78      0.80        18\n",
      "     neutral       0.80      0.57      0.67        14\n",
      "    positive       0.65      0.83      0.73        18\n",
      "\n",
      "    accuracy                           0.74        50\n",
      "   macro avg       0.76      0.73      0.73        50\n",
      "weighted avg       0.76      0.74      0.74        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in my_tweets.items():\n",
    "    the_tweet = tweet_info['text_of_tweet']\n",
    "    vader_output = run_vader(the_tweet, lemmatize=to_lemmatize) # run vader\n",
    "    vader_label = vader_output_to_label(vader_output) # convert vader output to category\n",
    "    \n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info['sentiment_label'])\n",
    "    print('Tweet:', id_, the_tweet)\n",
    "    print('Vader label:', vader_label)\n",
    "    print('Gold label:', tweet_info['sentiment_label'])\n",
    "    print()\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "report = classification_report(gold, all_vader_output, digits=2)\n",
    "print('a:', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "b) **We analysed all the incorrectly classified**\n",
    "\n",
    "**SENTENCE 1:** The Northern Lights, an atmospheric phenomenon rarely seen in the Netherlands, were visible over large parts of the country on Sunday night.\n",
    "* ACTUAL(GOLD):  positive\n",
    "* VADER OUTPUT:  neutral\n",
    "* {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
    "\n",
    "No words are in the VADER lexicon so it classifies it as neutral. We thought ‚Äòphenomenon‚Äô would be rated positive which is why the gold label is positive. \n",
    "No difference when not lemmatizing.\n",
    "\n",
    "**SENTENCE 2:** actually cannot breathe from how wholesome this is, not a single bad thing happens in it & I was real scared for a minute there\n",
    "* ACTUAL(GOLD):  positive\n",
    "* VADER OUTPUT:  negative\n",
    "* {'neg': 0.113, 'neu': 0.777, 'pos': 0.111, 'compound': -0.0129}\n",
    "\n",
    "VADER picks up the word ‚Äòscared‚Äô and ‚Äònot‚Ä¶bad‚Äô and the subpart ‚Äònot‚Ä¶bad‚Äô has a significant positive impact on the compound rating (as there is the negation of the strongly negative word ‚Äòbad‚Äô). This balances the negative words and yields a neutral compound rating.\n",
    "We assessed this by removing that part of the sentence. When doing so the overall compound rating dropped heavily towards a negative rating. \n",
    "No difference when not lemmatizing.\n",
    "\n",
    "**SENTENCE 3:** there are like 4 ppl that hacked into my netflix from different countries so I just left them a message to try and organize a movie night with all of them\n",
    "* ACTUAL(GOLD):  neutral\n",
    "* VADER OUTPUT:  positive when Lemmatized, otherwise negative \n",
    "* Lemmatize = true {'neg': 0.051, 'neu': 0.863, 'pos': 0.086, 'compound': 0.2551}\n",
    "* Lemmatize = false {'neg': 0.089, 'neu': 0.828, 'pos': 0.083, 'compound': -0.0516}\n",
    "\n",
    "When lemmatized is set to true, the word ‚Äúlike‚Äù is picked up by VADER. In this context the word ‚Äúlike‚Äù is not used in a positive way but as a preposition but VADER doesn‚Äôt understand that, thus the sentence is rated positively. When lemmatizing is set to false, additionally the word ‚Äúhacked‚Äù is picked up, making the sentence negative.\n",
    "\n",
    "**SENTENCE 18:** Thousands took to Lisbon‚Äôs streets on Saturday to demand better living conditions at a time high inflation is making it even tougher for people to make ends meet. | @Reuters\n",
    "* ACTUAL(GOLD):  neutral\n",
    "* VADER OUTPUT:  positive\n",
    "* Lemmatize {'neg': 0.096, 'neu': 0.836, 'pos': 0.068, 'compound': 0.0258}\n",
    "* Not lemmatize {'neg': 0.047, 'neu': 0.81, 'pos': 0.143, 'compound': 0.4767}\n",
    "\n",
    "This gold label is neutral as this is an objective sentence from the news. When lemmatized, ‚Äúbetter‚Äù becomes ‚Äúwell‚Äù which has a lower positive rating than ‚Äúbetter‚Äù. Additionally, ‚Äútougher‚Äù becomes ‚Äútough‚Äù which has a negative rating while ‚Äútougher‚Äù has a positive rating. Thus, VADER gives a higher compound rating when the sentence is not lemmatized (‚Äúbetter and ‚Äútougher‚Äù is kept) and rates it positively.  \n",
    "\n",
    "\n",
    "**SENTENCE 20:** Last night, the northern lights were observed by residents of those countries where it is almost never seen.  An unusual phenomenon was seen by residents of Britain, Denmark, the Netherlands and the United States.Scientists explain everything by a recent solar flare.\n",
    "* ACTUAL(GOLD):  neutral\n",
    "* VADER OUTPUT:  positive\n",
    "* {'neg': 0.0, 'neu': 0.935, 'pos': 0.065, 'compound': 0.4215}\n",
    "\n",
    "This gold label is neutral because this is an objective sentence from the news. VADER picks up the word ‚Äúunited‚Äù as positive while in this context it is referred as a country. No difference when not lemmatized. \n",
    "\n",
    "\n",
    "**SENTENCE 22:** And she can demean the President of the United States at the SOTU speech....two faced POS üôÑ\n",
    "* ACTUAL(GOLD):  negative\n",
    "* VADER OUTPUT:  positive\n",
    "* {'neg': 0.0, 'neu': 0.859, 'pos': 0.141, 'compound': 0.4215}\n",
    "\n",
    "This gold label is negative because of the name-calling ‚Äútwo faced POS üôÑ‚Äù. VADER picks up ‚Äúunited‚Äù as positive and it didn‚Äôt pick up ‚ÄúPOS‚Äù or ‚ÄúüôÑ‚Äù which were the main reason for the gold label. If the emoticon was changed to ‚Äú:/‚Äù, the sentence would be classified as negative. Same goes for changing ‚ÄúPOS‚Äù to ‚Äúpiece of shit‚Äù where VADER then picks up ‚Äúshit‚Äù. No difference when not lemmatized.\n",
    "\n",
    "**SENTENCE 27:** So i tested positive for COVID. Continued prayers please.\n",
    "* ACTUAL(GOLD):  negative\n",
    "* VADER OUTPUT:  positive\n",
    "* {'neg': 0.0, 'neu': 0.493, 'pos': 0.507, 'compound': 0.7334}\n",
    "\n",
    "This gold label is negative as it is about getting tested positive for a disease. VADER picks up ‚Äúpositive‚Äù and ‚Äúplease‚Äù which are both positively rated words making the overall rating positive. No difference when not lemmatized.\n",
    "\n",
    "\n",
    "**SENTENCE 31:** After a Marilyn Manson accuser claimed Evan Rachel Wood \"manipulated\" her, the actress provided a voicemail from the accuser saying she believed the rocker's attorney wanted the accuser to ‚Äúturn on the other girls and say that it was all a ruse‚Äù\n",
    "* ACTUAL(GOLD):  negative\n",
    "* VADER OUTPUT:  positive\n",
    "* Lemmatize {'neg': 0.0, 'neu': 0.968, 'pos': 0.032, 'compound': 0.0772}\n",
    "* Not Lemmatize {'neg': 0.062, 'neu': 0.938, 'pos': 0.0, 'compound': -0.3818}\n",
    "\n",
    "This gold label is negative because it involves manipulation and arguments. When lemmatized, ‚Äúmanipulated‚Äù becomes ‚Äúmanipulate‚Äù which is not in the lexicon. ‚ÄúWanted‚Äù becomes ‚Äúwant‚Äù which is slightly positive while ‚Äúwanted‚Äù is not in the lexicon. When not lemmatized, the VADER output is also negative as it picks up ‚Äúmanipulated‚Äù and does not pick up ‚Äúwanted‚Äù.\n",
    "\n",
    "\n",
    "**SENTENCE 35:** Myspace Tom is a SAVAGE\n",
    "* ACTUAL(GOLD):  positive\n",
    "* VADER OUTPUT:  negative\n",
    "* {'neg': 0.5, 'neu': 0.5, 'pos': 0.0, 'compound': -0.4588}\n",
    "\n",
    "This gold label is positive because using savage to describe someone online is in a positive way. VADER picks up the word ‚Äúsavage‚Äù which is rated negatively. When lemmatized, ‚ÄúSAVAGE‚Äù becomes ‚Äúsavage‚Äù which makes the compound rating less negative because capitalisation increases the intensity of the negative word.**\n",
    "\n",
    "**SENTENCE 36:** Disney World Fortifies Borders With Armed Characters As Park Announces Plan To Secede From Florida\n",
    "* ACTUAL(GOLD):  negative\n",
    "* VADER OUTPUT:  neutral\n",
    "* {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
    "\n",
    "This gold label is negative because of the word ‚Äúfortifies‚Äù as it has a connection to military and the word ‚Äúsecede‚Äù which has a connection with seperation. VADER doesn‚Äôt pick up any words. No difference when not lemmatized.\n",
    "\n",
    "\n",
    "**SENTENCE 37:** What is carbon capture and how does it fight climate change?\n",
    "* ACTUAL(GOLD):  neutral\n",
    "* VADER OUTPUT:  negative\n",
    "* {'neg': 0.206, 'neu': 0.794, 'pos': 0.0, 'compound': -0.3818}\n",
    "\n",
    "This gold label is neutral because it is a informative question. VADER picks up ‚Äúfight‚Äù which is rated negatively. No difference when not lemmatized.\n",
    "\n",
    "\n",
    "**SENTENCE 48:** safety is our number one priority\n",
    "* ACTUAL(GOLD):  neutral\n",
    "* VADER OUTPUT:  positive\n",
    "* {'neg': 0.0, 'neu': 0.494, 'pos': 0.506, 'compound': 0.4767}\n",
    "\n",
    "This gold label is neutral because of the objective declaration. VADER picks up ‚Äúsafety‚Äù which is very positive and ‚Äúnumber‚Äù which is slightly positive rating the overall sentence positively. No difference when not lemmatized.\n",
    "\n",
    "**SENTENCE 50:** russian boats need to be removed from the Black Sea.  Give Ukraine the ability to free the Black Sea Corridor.\n",
    "* ACTUAL(GOLD):  neutral\n",
    "* VADER OUTPUT:  positive\n",
    "* {'neg': 0.0, 'neu': 0.763, 'pos': 0.237, 'compound': 0.6808}\n",
    "\n",
    "This gold label is neutral because. VADER picks up ‚Äúability‚Äù and ‚Äúfree‚Äù which are both rated positively rating the overall sentence also positively. No difference when not lemmatized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 4:\n",
    "Run VADER on the set of airline tweets with the following settings:\n",
    "\n",
    "* Run VADER (as it is) on the set of airline tweets \n",
    "* Run VADER on the set of airline tweets after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only adjectives\n",
    "* Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only nouns\n",
    "* Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only verbs\n",
    "* Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "\n",
    "* [1 point] a. Generate for all separate experiments the classification report, i.e., Precision, Recall, and F<sub>1</sub> scores per category as well as micro and macro averages. **Use a different code cell (or multiple code cells) for each experiment.**\n",
    "* [3 points] b. Compare the scores and explain what they tell you.\n",
    "* - Does lemmatisation help? Explain why or why not.\n",
    "* - Are all parts of speech equally important for sentiment analysis? Explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "cwd = pathlib.Path.cwd()\n",
    "airline_tweets_folder = cwd.joinpath('airlinetweets')\n",
    "airline_tweets = load_files(str(airline_tweets_folder))\n",
    "\n",
    "vader_labels = []\n",
    "gold_labels = []\n",
    "\n",
    "def run_vader_airline(tweets, lemmatize=False, pos=set()):\n",
    "    \n",
    "    for tweet, label in tweets(): ####\n",
    "        the_tweet = ####\n",
    "    \n",
    "        vader_output = run_vader(the_tweet, lemmatize, pos)\n",
    "        vader_label = vader_output_to_label(vader_output)\n",
    "    \n",
    "        vader_labels.append(vader_label)\n",
    "        gold_labels.append(airline_tweets.target_names[label])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: scikit-learn assignments\n",
    "### [4 points] Question 5\n",
    "Train the scikit-learn classifier (Naive Bayes) using the airline tweets.\n",
    "\n",
    "+ Train the model on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
    "+ Train with different settings:\n",
    "    + with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count') \n",
    "    + with respect to the frequency threshold (min_df). Carry out experiments with increasing values for document frequency (min_df = 2; min_df = 5; min_df =10) \n",
    "* [1 point] a. Generate a classification_report for all experiments\n",
    "* [3 points] b. Look at the results of the experiments with the different settings and try to explain why they differ: \n",
    "    + which category performs best, is this the case for any setting?\n",
    "    + does the frequency threshold affect the scores? Why or why not according to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 6: Inspecting the best scoring features \n",
    "\n",
    "+ Train the scikit-learn classifier (Naive Bayes) model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "* [1 point] a. Generate the list of best scoring features per class (see function **important_features_per_class** below) [1 point]\n",
    "* [3 points] b. Look at the lists and consider the following issues: \n",
    "    + [1 point] Which features did you expect for each separate class and why?\n",
    "    + [1 point] Which features did you not expect and why ? \n",
    "    + [1 point] The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def important_features_per_class(vectorizer,classifier,n=80):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat) \n",
    "\n",
    "# example of how to call from notebook:\n",
    "#important_features_per_class(airline_vec, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not  be graded)] Question 7\n",
    "Train the model on airline tweets and test it on your own set of tweets\n",
    "+ Train the model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "+ Apply the model on your own set of tweets and generate the classification report\n",
    "* [1 point] a. Carry out a quantitative analysis.\n",
    "* [1 point] b. Carry out an error analysis on 10 correctly and 10 incorrectly classified tweets and discuss them\n",
    "* [2 points] c. Compare the results (cf. classification report) with the results obtained by VADER on the same tweets and discuss the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not be graded)] Question 8: trying to improve the model\n",
    "* [2 points] a. Think of some ways to improve the scikit-learn Naive Bayes model by playing with the settings or applying linguistic preprocessing (e.g., by filtering on part-of-speech, or removing punctuation). Do not change the classifier but continue using the Naive Bayes classifier. Explain what the effects might be of these other settings \n",
    "+ [1 point] b. Apply the model with at least one new setting (train on the airline tweets using 80% training, 20% test) and generate the scores\n",
    "* [1 point] c. Discuss whether the model achieved what you expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
