{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 - Assignment Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the LAB-2 assignment of the Text Mining course. It is about sentiment analysis.\n",
    "\n",
    "The aims of the assignment are:\n",
    "* Learn how to run a rule-based sentiment analysis module (VADER)\n",
    "* Learn how to run a machine learning sentiment analysis module (Scikit-Learn/ Naive Bayes)\n",
    "* Learn how to run scikit-learn metrics for the quantitative evaluation\n",
    "* Learn how to perform and interpret a quantitative evaluation of the outcomes of the tools (in terms of Precision, Recall, and F<sub>1</sub>)\n",
    "* Learn how to evaluate the results qualitatively (by examining the data) \n",
    "* Get insight into differences between the two applied methods\n",
    "* Get insight into the effects of using linguistic preprocessing\n",
    "* Be able to describe differences between the two methods in terms of their results\n",
    "* Get insight into issues when applying these methods across different  domains\n",
    "\n",
    "In this assignment, you are going to create your own gold standard set from 50 tweets. You will the VADER and scikit-learn classifiers to these tweets and evaluate the results by using evaluation metrics and inspecting the data.\n",
    "\n",
    "We recommend you go through the notebooks in the following order:\n",
    "* **Read the assignment (see below)**\n",
    "* **Lab3.2-Sentiment-analysis-with-VADER.ipynb**\n",
    "* **Lab3.3-Sentiment-analysis.with-scikit-learn.ipynb**\n",
    "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
    "\n",
    "In this assignment you are asked to perform both quantitative evaluations and error analyses:\n",
    "* a quantitative evaluation concerns the scores (Precision, Recall, and F<sub>1</sub>) provided by scikit's classification_report. It includes the scores per category, as well as micro and macro averages. Discuss whether the scores are balanced or not between the different categories (positive, negative, neutral) and between precision and recall. Discuss the shortcomings (if any) of the classifier based on these scores\n",
    "* an error analysis regarding the misclassifications of the classifier. It involves going through the texts and trying to understand what has gone wrong. It servers to get insight in what could be done to improve the performance of the classifier. Do you observe patterns in misclassifications?  Discuss why these errors are made and propose ways to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "The notebooks in this block have been originally created by [Marten Postma](https://martenpostma.github.io) and [Isa Maks](https://research.vu.nl/en/persons/e-maks). Adaptations were made by [Filip Ilievski](http://ilievski.nl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: VADER assignments\n",
    "\n",
    "\n",
    "### Preparation (nothing to submit):\n",
    "To be able to answer the VADER questions you need to know how the tool works. \n",
    "* Read more about the VADER tool in [this blog](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html).  \n",
    "* VADER provides 4 scores (positive, negative, neutral, compound). Be sure to understand what they mean and how they are calculated.\n",
    "* VADER uses rules to handle linguistic phenomena such as negation and intensification. Be sure to understand which rules are used, how they work, and why they are important.\n",
    "* VADER makes use of a sentiment lexicon. Have a look at the lexicon. Be sure to understand which information can be found there (lemma?, wordform?, part-of-speech?, polarity value?, word meaning?) What do all scores mean? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) \n",
    "\n",
    "\n",
    "### [3.5 points] Question1:\n",
    "\n",
    "Regard the following sentences and their output as given by VADER. Regard sentences 1 to 7, and explain the outcome **for each sentence**. Take into account both the rules applied by VADER and the lexicon that is used. You will find that some of the results are reasonable, but others are not. Explain what is going wrong or not when correct and incorrect results are produced. \n",
    "\n",
    "```\n",
    "INPUT SENTENCE 1 I love apples\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
    "\n",
    "INPUT SENTENCE 2 I don't love apples\n",
    "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
    "\n",
    "INPUT SENTENCE 3 I love apples :-)\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
    "\n",
    "INPUT SENTENCE 4 These houses are ruins\n",
    "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
    "\n",
    "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
    "\n",
    "INPUT SENTENCE 6 He lies in the chair in the garden\n",
    "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
    "\n",
    "INPUT SENTENCE 7 This house is like any house\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence 1\n",
    "Sentence 1 produced a label of positive, with it being 0% negative, 19.2% neutral, and 80.8% positive. The reason for this is due to the word \"love\", which has a sentiment rating of 3.2.\n",
    "\n",
    "### Sentence 2\n",
    "Sentence two has an overall label of negative, with it being 62.7% negative, 37.3% neutral and 0% positive. The reason for this is due to the word \"don't\", which switches the sentiment from positive to negative.\n",
    "\n",
    "### Sentence 3\n",
    "Sentence 3 has an overall label of positive, with it being 0% negative, 13.3% neutral and 86.7% positive. The sentence is considered to be more positive than sentence 1 due to the fact that it has an emoticon with a sentiment rating of 1.3. \n",
    "\n",
    "### Sentence 4\n",
    "Sentence 4 produced scores of 50.8% neutral, 49.2% negative, and 0% positive. While the score of neutral is higher than that of negative, the split is very close to equal. The word \"ruins\" has a sentiment rating of -1.9, which makes VADER's output more negative, which results in the overall score of -0.4404. \n",
    "\n",
    "### Sentence 5\n",
    "Sentence 4 produced scores of 0% negative, 51% neutral, and 49% positive, meaning that it is almost equally split between neutral and positive. The words \"certainly not\" negate the negative sentiment of the word \"ruins\", which results in a sentence which is scored as positive rather than negative. \n",
    "\n",
    "### Sentence 6\n",
    "Sentence 6 is 28.6% negative, 71.4% neutral, and 0% positive. This sentence seems to be incorrectly labeled as negative due to the fact that the word \"lies\" is used, which has a sentiment score of -1.8. The sentence should be labelled as neutral, but due to the fact that \"lies\" can also be used in the context of lying to someone, it is labelled as negative. \n",
    "\n",
    "### Sentence 7\n",
    "Sentence 7 is 0% negative, 66.7% neutral, and 33.3% positive, with a overall label of positive. This seems to be incorret, as this sentence is using the word \"like\" as a way to compare the house, and that it is not unique. This is done due to the word \"like\", as stated, which has a sentiment rating of 1.5, but considering the context in which it is used, this is incorrect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Points: 2.5] Exercise 2: Collecting 50 tweets for evaluation\n",
    "Collect 50 tweets. Try to find tweets that are interesting for sentiment analysis, e.g., very positive, neutral, and negative tweets. These could be your own tweets (typed in) or collected from the Twitter stream.\n",
    "\n",
    "We will store the tweets in the file **my_tweets.json** (use a text editor to edit).\n",
    "For each tweet, you should insert:\n",
    "* sentiment analysis label: negative | neutral | positive (this you determine yourself, this is not done by a computer)\n",
    "* the text of the tweet\n",
    "* the Tweet-URL\n",
    "\n",
    "from:\n",
    "```\n",
    "    \"1\": {\n",
    "        \"sentiment_label\": \"\",\n",
    "        \"text_of_tweet\": \"\",\n",
    "        \"tweet_url\": \"\",\n",
    "```\n",
    "to:\n",
    "```\n",
    "\"1\": {\n",
    "        \"sentiment_label\": \"positive\",\n",
    "        \"text_of_tweet\": \"All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\",\n",
    "        \"tweet_url\" : \"https://twitter.com/BarackObama/status/946775615893655552\",\n",
    "    },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load your tweets with human annotation in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sklearn\n",
    "import pathlib\n",
    "import spacy\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_files\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import classification_report\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "vader_model = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweets = json.load(open('my_tweets.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'sentiment_label': 'positive', 'text_of_tweet': 'The Northern Lights, an atmospheric phenomenon rarely seen in the Netherlands, were visible over large parts of the country on Sunday night.', 'tweet_url': 'https://twitter.com/DutchNewsNL/status/1630281109274599425'}\n"
     ]
    }
   ],
   "source": [
    "for id_, tweet_info in my_tweets.items():\n",
    "    print(id_, tweet_info)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Question 3:\n",
    "\n",
    "Run VADER on your own tweets (see function **run_vader** from notebook **Lab2-Sentiment-analysis-using-VADER.ipynb**). You can use the code snippet below this explanation as a starting point. \n",
    "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
    "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vader(textual_unit, \n",
    "              lemmatize=False,\n",
    "              parts_of_speech_to_consider=set(),\n",
    "              verbose=0):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence from spacy\n",
    "    \n",
    "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
    "    (by looping over doc.sents)\n",
    "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
    "    :param set parts_of_speech_to_consider:\n",
    "    -empty set -> all parts of speech are provided\n",
    "    -non-empty set: only these parts of speech are considered\n",
    "    :param int verbose: if set to 1, information is printed\n",
    "    about input and output\n",
    "    \n",
    "    :rtype: dict\n",
    "    :return: vader output dict\n",
    "    \"\"\"\n",
    "    doc = nlp(textual_unit)\n",
    "        \n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == '-PRON-': \n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add) \n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print()\n",
    "        print('INPUT SENTENCE', sent)\n",
    "        print('INPUT TO VADER', input_to_vader)\n",
    "        print('VADER OUTPUT', scores)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.824     0.778     0.800        18\n",
      "     neutral      0.800     0.571     0.667        14\n",
      "    positive      0.652     0.833     0.732        18\n",
      "\n",
      "    accuracy                          0.740        50\n",
      "   macro avg      0.759     0.728     0.733        50\n",
      "weighted avg      0.755     0.740     0.738        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in my_tweets.items():\n",
    "    the_tweet = tweet_info['text_of_tweet']\n",
    "    vader_output = run_vader(the_tweet, lemmatize = to_lemmatize) # run vader\n",
    "    vader_label = vader_output_to_label(vader_output) # convert vader output to category\n",
    "    \n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info['sentiment_label'])\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "report = classification_report(gold,all_vader_output,digits = 3)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2.5 points]** a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
    "\n",
    "The table above shows the output of the quantitative evaluation. The precision shows the ratio of true positives divided by the true positives and false positives; the recall is the ratio of true positives divided by the true positives and false negatives; the f1-score is the weighted mean of the test's precision and recall. The most accurate labeling for the precision was the negative sentiment tweets; for the recall, it was the positive tweets; and for the f1-score, it is the negative tweets. \n",
    "\n",
    "# [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 4:\n",
    "Run VADER on the set of airline tweets with the following settings:\n",
    "\n",
    "* Run VADER (as it is) on the set of airline tweets \n",
    "* Run VADER on the set of airline tweets after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only adjectives\n",
    "* Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only nouns\n",
    "* Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only verbs\n",
    "* Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "\n",
    "* [1 point] a. Generate for all separate experiments the classification report, i.e., Precision, Recall, and F<sub>1</sub> scores per category as well as micro and macro averages. **Use a different code cell (or multiple code cells) for each experiment.**\n",
    "* [3 points] b. Compare the scores and explain what they tell you.\n",
    "* - Does lemmatisation help? Explain why or why not.\n",
    "* - Are all parts of speech equally important for sentiment analysis? Explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Load airline tweet files:\n",
    "cwd = pathlib.Path.cwd()\n",
    "airline_tweets_folder = cwd.joinpath('airlinetweets')\n",
    "airline_tweets= load_files(str(airline_tweets_folder))\n",
    "\n",
    "# Function for running VADER with different settings:\n",
    "def vader_tweets(tweets,\n",
    "                        lemmatize_value=False,\n",
    "                        pos_value=set()):\n",
    "    vader_label =[]\n",
    "    gold = []\n",
    "\n",
    "    for tweet, label_int in zip(tweets.data, tweets.target):\n",
    "        tweet_ = tweet.decode(\"utf-8\")\n",
    "        vader_output = run_vader(tweet_, lemmatize = lemmatize_value, parts_of_speech_to_consider = pos_value)\n",
    "        vader_output_label = vader_output_to_label(vader_output)\n",
    "        vader_label.append(vader_output_label)\n",
    "        gold_label = airline_tweets.target_names[label_int]\n",
    "        gold.append(gold_label)\n",
    "    report = classification_report(gold, vader_label, digits = 3)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[1 point]** a. Generate for all separate experiments the classification report, i.e., Precision, Recall, and F1 scores per category as well as micro and macro averages. Use a different code cell (or multiple code cells) for each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.797     0.515     0.625      1750\n",
      "     neutral      0.605     0.506     0.551      1515\n",
      "    positive      0.559     0.884     0.685      1490\n",
      "\n",
      "    accuracy                          0.628      4755\n",
      "   macro avg      0.654     0.635     0.621      4755\n",
      "weighted avg      0.661     0.628     0.620      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run VADER (as it is) on the set of airline tweets\n",
    "vader_tweets(airline_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.786     0.522     0.628      1750\n",
      "     neutral      0.598     0.488     0.538      1515\n",
      "    positive      0.557     0.881     0.682      1490\n",
      "\n",
      "    accuracy                          0.624      4755\n",
      "   macro avg      0.647     0.630     0.616      4755\n",
      "weighted avg      0.654     0.624     0.616      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run VADER on the set of airline tweets after having lemmatized the text\n",
    "vader_tweets(airline_tweets, lemmatize_value = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.870     0.210     0.339      1750\n",
      "     neutral      0.403     0.892     0.555      1515\n",
      "    positive      0.665     0.438     0.528      1490\n",
      "\n",
      "    accuracy                          0.499      4755\n",
      "   macro avg      0.646     0.513     0.474      4755\n",
      "weighted avg      0.657     0.499     0.467      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run VADER on the set of airline tweets with only adjectives\n",
    "vader_tweets(airline_tweets, pos_value = {'ADJ'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.868     0.210     0.339      1750\n",
      "     neutral      0.403     0.892     0.556      1515\n",
      "    positive      0.664     0.438     0.528      1490\n",
      "\n",
      "    accuracy                          0.499      4755\n",
      "   macro avg      0.645     0.513     0.474      4755\n",
      "weighted avg      0.656     0.499     0.467      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "vader_tweets(airline_tweets, lemmatize_value = True,pos_value = {'ADJ'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.730     0.143     0.240      1750\n",
      "     neutral      0.358     0.817     0.498      1515\n",
      "    positive      0.532     0.340     0.415      1490\n",
      "\n",
      "    accuracy                          0.420      4755\n",
      "   macro avg      0.540     0.433     0.384      4755\n",
      "weighted avg      0.549     0.420     0.377      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run VADER on the set of airline tweets with only nouns\n",
    "vader_tweets(airline_tweets, pos_value = {'NOUN'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.715     0.157     0.257      1750\n",
      "     neutral      0.358     0.809     0.496      1515\n",
      "    positive      0.521     0.331     0.405      1490\n",
      "\n",
      "    accuracy                          0.419      4755\n",
      "   macro avg      0.531     0.432     0.386      4755\n",
      "weighted avg      0.540     0.419     0.379      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "vader_tweets(airline_tweets, lemmatize_value = True,pos_value = {'NOUN'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.774     0.288     0.420      1750\n",
      "     neutral      0.383     0.810     0.520      1515\n",
      "    positive      0.568     0.343     0.428      1490\n",
      "\n",
      "    accuracy                          0.472      4755\n",
      "   macro avg      0.575     0.480     0.456      4755\n",
      "weighted avg      0.585     0.472     0.454      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run VADER on the set of airline tweets with only verbs\n",
    "vader_tweets(airline_tweets, pos_value = {'VERB'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.741     0.295     0.422      1750\n",
      "     neutral      0.377     0.780     0.508      1515\n",
      "    positive      0.568     0.352     0.434      1490\n",
      "\n",
      "    accuracy                          0.468      4755\n",
      "   macro avg      0.562     0.476     0.455      4755\n",
      "weighted avg      0.571     0.468     0.454      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "vader_tweets(airline_tweets,lemmatize_value = True, pos_value = {'VERB'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3 points]** b. Compare the scores and explain what they tell you.\n",
    "\n",
    "1. Does lemmatisation help? Explain why or why not.\n",
    "\n",
    "When looking at the results of the classification reports, it seems to show that lemmatisation does not make a large noticable difference in this dataset. The tweets that have had lemmatisation applied appear to have a decrease in the precision and recall, as well as the macro average and weighted average. There are some aspects that have some slight improvements, such as the negative recall and positive f1-score for verbs. Lemmatisation results in the verb being used as its lemma (base form), and this could mean that the meaning of the verb could be modified or lost, which may explain the results. \n",
    "\n",
    "2. Are all parts of speech equally important for sentiment analysis? Explain why or why not.\n",
    "\n",
    "When comparing the recall, precision, and f1-scores, the results appear to be not significantly different. On the other hand, when looking at the macro and weighted averages, the results show a consistant decrease when looking at specific parts of speech. This difference can be explained by the loss of context when only analyzing one part of speech, as without the context, it can be difficult to tell what the intention of the writer is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: scikit-learn assignments\n",
    "### [4 points] Question 5\n",
    "Train the scikit-learn classifier (Naive Bayes) using the airline tweets.\n",
    "\n",
    "+ Train the model on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
    "+ Train with different settings:\n",
    "    + with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count') \n",
    "    + with respect to the frequency threshold (min_df). Carry out experiments with increasing values for document frequency (min_df = 2; min_df = 5; min_df =10) \n",
    "* [1 point] a. Generate a classification_report for all experiments\n",
    "* [3 points] b. Look at the results of the experiments with the different settings and try to explain why they differ: \n",
    "    + which category performs best, is this the case for any setting?\n",
    "    + does the frequency threshold affect the scores? Why or why not according to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import numpy\n",
    "import nltk\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vectorize_tweets(df, vectorizer):\n",
    "    airline_vectorizer = CountVectorizer(min_df = df, tokenizer = nltk.word_tokenize, stop_words = stopwords.words(\"english\")) \n",
    "    airline_counts = airline_vectorizer.fit_transform(airline_tweets.data)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "    \n",
    "    if vectorizer == \"tfidf\": \n",
    "        tweets_train, tweets_test, y_train, y_test = train_test_split(airline_tfidf, airline_tweets.target, test_size = 0.20)\n",
    "        tfidf_clf = MultinomialNB().fit(tweets_train, y_train)\n",
    "        tfidf_pred = tfidf_clf.predict(tweets_test)\n",
    "        report = sklearn.metrics.classification_report(y_true = y_test, y_pred = tfidf_pred, digits = 3)\n",
    "        \n",
    "    elif vectorizer == \"count\":\n",
    "        tweets_train, tweets_test, y_train, y_test = train_test_split(airline_counts, airline_tweets.target, test_size = 0.20)\n",
    "        count_clf = MultinomialNB().fit(tweets_train, y_train)\n",
    "        count_pred = count_clf.predict(tweets_test)\n",
    "        report = sklearn.metrics.classification_report(y_true = y_test, y_pred = count_pred, digits = 2)\n",
    "    else: \n",
    "        report = (\"%s vectorizer is not defined\" %vectorizer)\n",
    "        \n",
    "    return(report)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[1 point]** a. Generate a classification_report for all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snake/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/snake/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.794     0.913     0.850       334\n",
      "           2      0.838     0.639     0.725       324\n",
      "           3      0.781     0.853     0.816       293\n",
      "\n",
      "    accuracy                          0.801       951\n",
      "   macro avg      0.805     0.802     0.797       951\n",
      "weighted avg      0.805     0.801     0.797       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_vectorize_tweets(2,'tfidf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snake/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/snake/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.92      0.87       356\n",
      "           2       0.86      0.69      0.77       313\n",
      "           3       0.79      0.86      0.82       282\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.83      0.82      0.82       951\n",
      "weighted avg       0.83      0.83      0.82       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_vectorize_tweets(2,'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snake/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/snake/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.828     0.890     0.858       347\n",
      "           2      0.808     0.747     0.776       304\n",
      "           3      0.828     0.820     0.824       300\n",
      "\n",
      "    accuracy                          0.822       951\n",
      "   macro avg      0.822     0.819     0.820       951\n",
      "weighted avg      0.822     0.822     0.821       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_vectorize_tweets(5,'tfidf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snake/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/snake/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.825     0.864     0.844       360\n",
      "           2      0.804     0.764     0.783       296\n",
      "           3      0.857     0.851     0.854       295\n",
      "\n",
      "    accuracy                          0.829       951\n",
      "   macro avg      0.829     0.826     0.827       951\n",
      "weighted avg      0.828     0.829     0.828       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_vectorize_tweets(10,'tfidf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3 points]** b. Look at the results of the experiments with the different settings and try to explain why they differ:\n",
    "1. which category performs best, is this the case for any setting?\n",
    "\n",
    "The highest scoring category is 1, which is the negative category. For count, tfidf, and min_df of 2, 5, and 10 all have negative as the highest scoring on all metrics except for precision in certain cases. \n",
    "\n",
    "\n",
    "2. does the frequency threshold affect the scores? Why or why not according to you?\n",
    "\n",
    "The macro average, weighted average, and accuracy appear to increase when the frequency threshold is increased. There is also a tendency of the precision, recall, and f1-score to increase. This could be due to the fact that the remaining words/terms that are analyzed are more likely to contain a sentiment; this trend would likely not remain for a higher frequency threshold as too high of a frequency threshold could filter out too many terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 6: Inspecting the best scoring features \n",
    "\n",
    "+ Train the scikit-learn classifier (Naive Bayes) model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "* [1 point] a. Generate the list of best scoring features per class (see function **important_features_per_class** below) [1 point]\n",
    "* [3 points] b. Look at the lists and consider the following issues: \n",
    "    + [1 point] Which features did you expect for each separate class and why?\n",
    "    + [1 point] Which features did you not expect and why ? \n",
    "    + [1 point] The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snake/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/snake/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "airline_vectorizer = CountVectorizer(min_df=2, tokenizer = nltk.word_tokenize,stop_words=stopwords.words(\"english\"))\n",
    "airline_counts = airline_vectorizer.fit_transform(airline_tweets.data)\n",
    "\n",
    "tweets_train, tweets_test, y_train, y_test = train_test_split(airline_counts, airline_tweets.target, test_size = 0.20)\n",
    "    \n",
    "clf = MultinomialNB().fit(tweets_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[1 point]** a. Generate the list of best scoring features per class (see function important_features_per_class below) [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in negative documents\n",
      "1 1506.0 @\n",
      "1 1384.0 united\n",
      "1 1217.0 .\n",
      "1 414.0 ``\n",
      "1 403.0 flight\n",
      "1 386.0 ?\n",
      "1 374.0 !\n",
      "1 326.0 #\n",
      "1 207.0 n't\n",
      "1 156.0 ''\n",
      "1 131.0 's\n",
      "1 109.0 service\n",
      "1 109.0 :\n",
      "1 101.0 virginamerica\n",
      "1 97.0 cancelled\n",
      "1 96.0 bag\n",
      "1 92.0 customer\n",
      "1 90.0 get\n",
      "1 87.0 delayed\n",
      "1 83.0 time\n",
      "1 83.0 plane\n",
      "1 81.0 -\n",
      "1 76.0 'm\n",
      "1 75.0 ...\n",
      "1 73.0 ;\n",
      "1 70.0 http\n",
      "1 69.0 hours\n",
      "1 69.0 gate\n",
      "1 68.0 &\n",
      "1 61.0 hour\n",
      "1 60.0 still\n",
      "1 59.0 2\n",
      "1 58.0 would\n",
      "1 58.0 amp\n",
      "1 57.0 late\n",
      "1 55.0 airline\n",
      "1 54.0 help\n",
      "1 53.0 worst\n",
      "1 53.0 ca\n",
      "1 52.0 one\n",
      "1 51.0 like\n",
      "1 51.0 flights\n",
      "1 49.0 waiting\n",
      "1 47.0 never\n",
      "1 47.0 delay\n",
      "1 45.0 've\n",
      "1 44.0 flightled\n",
      "1 43.0 (\n",
      "1 42.0 back\n",
      "1 41.0 $\n",
      "1 40.0 us\n",
      "1 40.0 really\n",
      "1 40.0 3\n",
      "1 40.0 )\n",
      "1 38.0 lost\n",
      "1 37.0 seat\n",
      "1 37.0 ever\n",
      "1 36.0 u\n",
      "1 36.0 due\n",
      "1 36.0 check\n",
      "1 35.0 people\n",
      "1 34.0 trying\n",
      "1 34.0 seats\n",
      "1 34.0 fly\n",
      "1 34.0 airport\n",
      "1 33.0 wait\n",
      "1 33.0 hold\n",
      "1 32.0 thanks\n",
      "1 32.0 luggage\n",
      "1 32.0 got\n",
      "1 32.0 even\n",
      "1 32.0 day\n",
      "1 31.0 ticket\n",
      "1 31.0 last\n",
      "1 31.0 bags\n",
      "1 31.0 baggage\n",
      "1 31.0 another\n",
      "1 30.0 need\n",
      "1 30.0 4\n",
      "1 29.0 staff\n",
      "-----------------------------------------\n",
      "Important words in neutral documents\n",
      "2 1394.0 @\n",
      "2 511.0 ?\n",
      "2 498.0 .\n",
      "2 290.0 jetblue\n",
      "2 274.0 southwestair\n",
      "2 265.0 :\n",
      "2 262.0 united\n",
      "2 237.0 #\n",
      "2 235.0 ``\n",
      "2 227.0 flight\n",
      "2 190.0 americanair\n",
      "2 165.0 http\n",
      "2 159.0 usairways\n",
      "2 156.0 !\n",
      "2 129.0 's\n",
      "2 86.0 get\n",
      "2 71.0 -\n",
      "2 70.0 virginamerica\n",
      "2 67.0 flights\n",
      "2 61.0 please\n",
      "2 60.0 help\n",
      "2 60.0 )\n",
      "2 59.0 ''\n",
      "2 58.0 need\n",
      "2 52.0 n't\n",
      "2 52.0 (\n",
      "2 46.0 ;\n",
      "2 46.0 ...\n",
      "2 43.0 would\n",
      "2 41.0 tomorrow\n",
      "2 41.0 &\n",
      "2 39.0 dm\n",
      "2 37.0 us\n",
      "2 36.0 flying\n",
      "2 35.0 'm\n",
      "2 34.0 know\n",
      "2 34.0 amp\n",
      "2 33.0 change\n",
      "2 31.0 one\n",
      "2 30.0 “\n",
      "2 30.0 fly\n",
      "2 29.0 way\n",
      "2 29.0 thanks\n",
      "2 29.0 fleet\n",
      "2 29.0 fleek\n",
      "2 29.0 cancelled\n",
      "2 28.0 like\n",
      "2 28.0 hi\n",
      "2 27.0 ”\n",
      "2 27.0 could\n",
      "2 26.0 time\n",
      "2 26.0 airport\n",
      "2 25.0 go\n",
      "2 25.0 check\n",
      "2 24.0 today\n",
      "2 24.0 number\n",
      "2 23.0 see\n",
      "2 23.0 next\n",
      "2 23.0 new\n",
      "2 22.0 use\n",
      "2 22.0 travel\n",
      "2 22.0 going\n",
      "2 21.0 tickets\n",
      "2 21.0 ticket\n",
      "2 21.0 destinationdragons\n",
      "2 20.0 sent\n",
      "2 20.0 reservation\n",
      "2 20.0 guys\n",
      "2 20.0 first\n",
      "2 20.0 chance\n",
      "2 20.0 2\n",
      "2 19.0 trying\n",
      "2 19.0 morning\n",
      "2 19.0 make\n",
      "2 19.0 back\n",
      "2 19.0 add\n",
      "2 18.0 weather\n",
      "2 18.0 want\n",
      "2 18.0 still\n",
      "2 18.0 rt\n",
      "-----------------------------------------\n",
      "Important words in positive documents\n",
      "3 1358.0 @\n",
      "3 1065.0 !\n",
      "3 757.0 .\n",
      "3 316.0 #\n",
      "3 312.0 southwestair\n",
      "3 287.0 thanks\n",
      "3 281.0 jetblue\n",
      "3 264.0 united\n",
      "3 252.0 thank\n",
      "3 244.0 ``\n",
      "3 182.0 americanair\n",
      "3 181.0 flight\n",
      "3 172.0 :\n",
      "3 137.0 great\n",
      "3 136.0 usairways\n",
      "3 96.0 service\n",
      "3 83.0 )\n",
      "3 82.0 virginamerica\n",
      "3 77.0 http\n",
      "3 71.0 love\n",
      "3 71.0 customer\n",
      "3 69.0 best\n",
      "3 64.0 's\n",
      "3 63.0 much\n",
      "3 63.0 guys\n",
      "3 59.0 ;\n",
      "3 57.0 awesome\n",
      "3 51.0 airline\n",
      "3 50.0 amazing\n",
      "3 49.0 good\n",
      "3 44.0 time\n",
      "3 43.0 got\n",
      "3 43.0 -\n",
      "3 43.0 &\n",
      "3 41.0 n't\n",
      "3 39.0 today\n",
      "3 39.0 crew\n",
      "3 37.0 made\n",
      "3 37.0 help\n",
      "3 37.0 get\n",
      "3 36.0 fly\n",
      "3 36.0 amp\n",
      "3 35.0 us\n",
      "3 35.0 flying\n",
      "3 34.0 ...\n",
      "3 32.0 gate\n",
      "3 30.0 appreciate\n",
      "3 30.0 ''\n",
      "3 29.0 work\n",
      "3 29.0 home\n",
      "3 29.0 back\n",
      "3 29.0 'm\n",
      "3 27.0 see\n",
      "3 27.0 response\n",
      "3 27.0 ?\n",
      "3 26.0 new\n",
      "3 26.0 ever\n",
      "3 25.0 nice\n",
      "3 24.0 helpful\n",
      "3 23.0 plane\n",
      "3 23.0 like\n",
      "3 23.0 flights\n",
      "3 23.0 (\n",
      "3 22.0 yes\n",
      "3 22.0 well\n",
      "3 22.0 tonight\n",
      "3 22.0 southwest\n",
      "3 22.0 always\n",
      "3 22.0 're\n",
      "3 21.0 staff\n",
      "3 21.0 please\n",
      "3 21.0 know\n",
      "3 21.0 job\n",
      "3 21.0 agent\n",
      "3 21.0 'll\n",
      "3 19.0 wait\n",
      "3 19.0 team\n",
      "3 19.0 follow\n",
      "3 19.0 ..\n",
      "3 18.0 would\n"
     ]
    }
   ],
   "source": [
    "def important_features_per_class(vectorizer,classifier,n=80):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names_out()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat) \n",
    "\n",
    "# example of how to call from notebook:\n",
    "important_features_per_class(airline_vectorizer, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3 points]** b. Look at the lists and consider the following issues:\n",
    "1. [1 point] Which features did you expect for each separate class and why?\n",
    "\n",
    "In the negative documents, the words \"cancelled\", \"delayed\", and \"worst\" would be expected as they all carry a negative connotation. In the neutral documents, airline names, \"flights\", and non-specific words such as \"one\", \"tomorrow\", and \"would\" are expected as they do not contain any specific sentiment. In the positive documents, \"like\", \"nice\", \"amazing\", and \"love\", are to be expected as they all carry a positive connotation. \n",
    "\n",
    "2. [1 point] Which features did you not expect and why ?\n",
    "\n",
    "The word \"thanks\" was not expected in the negative documents as it is typically used in a positive context. The word \"thanks\" may have appeared in the negative documents if it was used in a sarcastic way. The exclamation mark in the neutral class was not expected as it is typically used in something with strong emotion.  \n",
    "\n",
    "3. [1 point] The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why?\n",
    "\n",
    "In order to improve the model, punctuation such as \"\"\", \"&\", \";\", \"-\", etc. should be removed because this does not contribute to the sentiment of the text. On the other hand, emoticons such as \":)\" should stay as they contain sentiments. Words with sentiment, such as \"cancel\", \"late\", \"worst\", etc., should also of course be kept, along with aspect words connected with the sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not  be graded)] Question 7\n",
    "Train the model on airline tweets and test it on your own set of tweets\n",
    "+ Train the model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "+ Apply the model on your own set of tweets and generate the classification report\n",
    "* [1 point] a. Carry out a quantitative analysis.\n",
    "* [1 point] b. Carry out an error analysis on 10 correctly and 10 incorrectly classified tweets and discuss them\n",
    "* [2 points] c. Compare the results (cf. classification report) with the results obtained by VADER on the same tweets and discuss the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not be graded)] Question 8: trying to improve the model\n",
    "* [2 points] a. Think of some ways to improve the scikit-learn Naive Bayes model by playing with the settings or applying linguistic preprocessing (e.g., by filtering on part-of-speech, or removing punctuation). Do not change the classifier but continue using the Naive Bayes classifier. Explain what the effects might be of these other settings \n",
    "+ [1 point] b. Apply the model with at least one new setting (train on the airline tweets using 80% training, 20% test) and generate the scores\n",
    "* [1 point] c. Discuss whether the model achieved what you expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
