{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4a Named-entity-recognition using fine-tuned transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before reading this notebook make sure you have consulted **Lab3.4 SentimentClassification using transformer models**, which contains some disclaimers, tips and explains the sentence representations obtained from the transformer models.\n",
    "\n",
    "In this notebook we will use the simpletransformer package that provides a simple API on top of the transformer packge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requires installing transformers, pytorch and simpletransformers\n",
    "#!conda install pytorch cpuonly -c pytorch\n",
    "#!pip install transformers\n",
    "#!pip install simpletransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a transformer model 'bert-base-NER' from the Hugging face repository, which is fine-tuned for Named Entity recognition: \n",
    "\n",
    "https://huggingface.co/models\n",
    "\n",
    "We need to load the model for the sequence classifcation and the tokenizer to convert the sentences into tokens according to the vocabulary of the model.\n",
    "\n",
    "Loading the model takes some time and requires you have sufficient memory to load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ago/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 829/829 [00:00<00:00, 818kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 433M/433M [01:06<00:00, 6.48MB/s] \n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 717kB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 2.00/2.00 [00:00<00:00, 1.09kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 52.3kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 59.0/59.0 [00:00<00:00, 39.0kB/s]\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.ner import NERModel\n",
    "#sentences = [\"Example sentence 1\", \"Example sentence 2\"]\n",
    "englishmodel = NERModel(\n",
    "        model_type=\"bert\",\n",
    "        model_name=\"dslim/bert-base-NER\",\n",
    "        use_cuda=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of the NERModel that can be used for training, evaluation, and prediction in Named-Entity-Recognition (NER) tasks. The full parameter list for a NERModel object:\n",
    "\n",
    "* model_type: The type of model (bert, roberta)\n",
    "* model_name: Default Transformer model name or path to a directory containing Transformer model file (pytorch_nodel.bin).\n",
    "* labels (optional): A list of all Named Entity labels. If not given, [“O”, “B-MISC”, “I-MISC”, “B-PER”, “I-PER”, “B-ORG”, “I-ORG”, “B-LOC”, “I-LOC”] will be used.\n",
    "* args (optional): Default args will be used if this parameter is not provided. If provided, it should be a dict containing the args that should be changed in the default args.\n",
    "* use_cuda (optional): Use GPU if available. Setting to False will force model to use CPU only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 125.50it/s]\n",
      "Running Prediction: 100%|██████████| 1/1 [00:00<00:00,  9.88it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions, raw_outputs = englishmodel.predict([\"Apple sued Samsung for patents last year.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'Apple': 'B-ORG'},\n",
       "  {'sued': 'O'},\n",
       "  {'Samsung': 'B-ORG'},\n",
       "  {'for': 'O'},\n",
       "  {'patents': 'O'},\n",
       "  {'last': 'O'},\n",
       "  {'year.': 'O'}]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.07k/1.07k [00:00<00:00, 756kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 434M/434M [02:07<00:00, 3.41MB/s] \n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 241k/241k [00:00<00:00, 260kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 194kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 546/546 [00:00<00:00, 200kB/s]\n"
     ]
    }
   ],
   "source": [
    "dutchmodel = NERModel(\n",
    "        model_type=\"bert\",\n",
    "        model_name=\"Matthijsvanhof/bert-base-dutch-cased-finetuned-NER\",\n",
    "        use_cuda=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 106.80it/s]\n",
      "Running Prediction: 100%|██████████| 1/1 [00:00<00:00,  9.85it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions, raw_outputs = dutchmodel.predict([\"Apple sleept Samsung voor de rechter vanwege schending van patenten.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'Apple': 'O'},\n",
       "  {'sleept': 'O'},\n",
       "  {'Samsung': 'B-MISC'},\n",
       "  {'voor': 'O'},\n",
       "  {'de': 'O'},\n",
       "  {'rechter': 'O'},\n",
       "  {'vanwege': 'O'},\n",
       "  {'schending': 'O'},\n",
       "  {'van': 'O'},\n",
       "  {'patenten.': 'O'}]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option for Dutch NER (https://huggingface.co/flair/ner-dutch-large):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidVersion",
     "evalue": "Invalid version: '2.22.1ubuntu1' (package: devscripts)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidVersion\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mflair\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Sentence\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mflair\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m SequenceTagger\n\u001b[1;32m      4\u001b[0m \u001b[39m# load tagger\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tagger \u001b[39m=\u001b[39m SequenceTagger\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mflair/ner-dutch-large\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/flair/__init__.py:25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m# global variable: arrow symbol\u001b[39;00m\n\u001b[1;32m     23\u001b[0m _arrow \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m → \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \u001b[39m# noqa: E402 import after setting device\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     data,\n\u001b[1;32m     27\u001b[0m     models,\n\u001b[1;32m     28\u001b[0m     nn,\n\u001b[1;32m     29\u001b[0m     trainers,\n\u001b[1;32m     30\u001b[0m     visual,\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m logging\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdictConfig(\n\u001b[1;32m     34\u001b[0m     {\n\u001b[1;32m     35\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     }\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m\"\u001b[39m\u001b[39mflair\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/flair/models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mclustering\u001b[39;00m \u001b[39mimport\u001b[39;00m ClusteringModel\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mentity_linker_model\u001b[39;00m \u001b[39mimport\u001b[39;00m EntityLinker\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlanguage_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LanguageModel\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/flair/models/clustering.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mflair\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Corpus, _iter_dataset\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mflair\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mflair\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membeddings\u001b[39;00m \u001b[39mimport\u001b[39;00m DocumentEmbeddings\n\u001b[1;32m     16\u001b[0m log \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m\"\u001b[39m\u001b[39mflair\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/flair/datasets/__init__.py:151\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mentity_linking\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m    134\u001b[0m     NEL_ENGLISH_AIDA,\n\u001b[1;32m    135\u001b[0m     NEL_ENGLISH_AQUAINT,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ZELDA,\n\u001b[1;32m    148\u001b[0m )\n\u001b[1;32m    150\u001b[0m \u001b[39m# Expose all relation extraction datasets\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mocr\u001b[39;00m \u001b[39mimport\u001b[39;00m SROIE, OcrJsonDataset\n\u001b[1;32m    152\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mrelation_extraction\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m    153\u001b[0m     RE_ENGLISH_CONLL04,\n\u001b[1;32m    154\u001b[0m     RE_ENGLISH_DRUGPROT,\n\u001b[1;32m    155\u001b[0m     RE_ENGLISH_SEMEVAL2010,\n\u001b[1;32m    156\u001b[0m     RE_ENGLISH_TACRED,\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    159\u001b[0m \u001b[39m# universal proposition banks\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m# keyphrase detection datasets\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m# other NER datasets\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m# standard NER datasets\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m# Expose all sequence labeling datasets\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/flair/datasets/ocr.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Dict, Optional, Union\n\u001b[0;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgdown\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdownload_folder\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mPIL\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gdown/__init__.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mextractall\u001b[39;00m \u001b[39mimport\u001b[39;00m extractall\n\u001b[1;32m     11\u001b[0m __author__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mKentaro Wada <www.kentaro.wada@gmail.com>\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m __version__ \u001b[39m=\u001b[39m pkg_resources\u001b[39m.\u001b[39;49mget_distribution(\u001b[39m\"\u001b[39;49m\u001b[39mgdown\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mversion\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:514\u001b[0m, in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    512\u001b[0m     dist \u001b[39m=\u001b[39m Requirement\u001b[39m.\u001b[39mparse(dist)\n\u001b[1;32m    513\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dist, Requirement):\n\u001b[0;32m--> 514\u001b[0m     dist \u001b[39m=\u001b[39m get_provider(dist)\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dist, Distribution):\n\u001b[1;32m    516\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mExpected string, Requirement, or Distribution\u001b[39m\u001b[39m\"\u001b[39m, dist)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:386\u001b[0m, in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(moduleOrReq, Requirement):\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mreturn\u001b[39;00m working_set\u001b[39m.\u001b[39mfind(moduleOrReq) \u001b[39mor\u001b[39;00m require(\u001b[39mstr\u001b[39;49m(moduleOrReq))[\u001b[39m0\u001b[39m]\n\u001b[1;32m    387\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     module \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mmodules[moduleOrReq]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:956\u001b[0m, in \u001b[0;36mWorkingSet.require\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequire\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mrequirements):\n\u001b[1;32m    948\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Ensure that distributions matching `requirements` are activated\u001b[39;00m\n\u001b[1;32m    949\u001b[0m \n\u001b[1;32m    950\u001b[0m \u001b[39m    `requirements` must be a string or a (possibly-nested) sequence\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[39m    included, even if they were already activated in this working set.\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 956\u001b[0m     needed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresolve(parse_requirements(requirements))\n\u001b[1;32m    958\u001b[0m     \u001b[39mfor\u001b[39;00m dist \u001b[39min\u001b[39;00m needed:\n\u001b[1;32m    959\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd(dist)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:815\u001b[0m, in \u001b[0;36mWorkingSet.resolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m req_extras\u001b[39m.\u001b[39mmarkers_pass(req, extras):\n\u001b[1;32m    813\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 815\u001b[0m dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resolve_dist(\n\u001b[1;32m    816\u001b[0m     req, best, replace_conflicting, env, installer, required_by, to_activate\n\u001b[1;32m    817\u001b[0m )\n\u001b[1;32m    819\u001b[0m \u001b[39m# push the new requirements onto the stack\u001b[39;00m\n\u001b[1;32m    820\u001b[0m new_requirements \u001b[39m=\u001b[39m dist\u001b[39m.\u001b[39mrequires(req\u001b[39m.\u001b[39mextras)[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:844\u001b[0m, in \u001b[0;36mWorkingSet._resolve_dist\u001b[0;34m(self, req, best, replace_conflicting, env, installer, required_by, to_activate)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[39mif\u001b[39;00m env \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    843\u001b[0m     \u001b[39mif\u001b[39;00m dist \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 844\u001b[0m         env \u001b[39m=\u001b[39m Environment(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mentries)\n\u001b[1;32m    845\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    846\u001b[0m         \u001b[39m# Use an empty environment and workingset to avoid\u001b[39;00m\n\u001b[1;32m    847\u001b[0m         \u001b[39m# any further conflicts with the conflicting\u001b[39;00m\n\u001b[1;32m    848\u001b[0m         \u001b[39m# distribution\u001b[39;00m\n\u001b[1;32m    849\u001b[0m         env \u001b[39m=\u001b[39m Environment([])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:1044\u001b[0m, in \u001b[0;36mEnvironment.__init__\u001b[0;34m(self, search_path, platform, python)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplatform \u001b[39m=\u001b[39m platform\n\u001b[1;32m   1043\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython \u001b[39m=\u001b[39m python\n\u001b[0;32m-> 1044\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan(search_path)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:1077\u001b[0m, in \u001b[0;36mEnvironment.scan\u001b[0;34m(self, search_path)\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m search_path:\n\u001b[1;32m   1076\u001b[0m     \u001b[39mfor\u001b[39;00m dist \u001b[39min\u001b[39;00m find_distributions(item):\n\u001b[0;32m-> 1077\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd(dist)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:1096\u001b[0m, in \u001b[0;36mEnvironment.add\u001b[0;34m(self, dist)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[39mif\u001b[39;00m dist \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m dists:\n\u001b[1;32m   1095\u001b[0m     dists\u001b[39m.\u001b[39mappend(dist)\n\u001b[0;32m-> 1096\u001b[0m     dists\u001b[39m.\u001b[39;49msort(key\u001b[39m=\u001b[39;49moperator\u001b[39m.\u001b[39;49mattrgetter(\u001b[39m'\u001b[39;49m\u001b[39mhashcmp\u001b[39;49m\u001b[39m'\u001b[39;49m), reverse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:2639\u001b[0m, in \u001b[0;36mDistribution.hashcmp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2636\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m   2637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhashcmp\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   2638\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m-> 2639\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparsed_version,\n\u001b[1;32m   2640\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecedence,\n\u001b[1;32m   2641\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey,\n\u001b[1;32m   2642\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocation,\n\u001b[1;32m   2643\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpy_version \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   2644\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplatform \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   2645\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:2693\u001b[0m, in \u001b[0;36mDistribution.parsed_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2691\u001b[0m             ex\u001b[39m.\u001b[39madd_note(info)  \u001b[39m# PEP 678\u001b[39;00m\n\u001b[1;32m   2692\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m-> 2693\u001b[0m         \u001b[39mraise\u001b[39;00m packaging\u001b[39m.\u001b[39mversion\u001b[39m.\u001b[39mInvalidVersion(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(ex)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00minfo\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m   2695\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parsed_version\n",
      "\u001b[0;31mInvalidVersion\u001b[0m: Invalid version: '2.22.1ubuntu1' (package: devscripts)"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# load tagger\n",
    "tagger = SequenceTagger.load(\"flair/ner-dutch-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tagger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m sentence \u001b[39m=\u001b[39m Sentence(\u001b[39m\"\u001b[39m\u001b[39mApple sleept Samsung voor de rechter vanwege schending van patenten.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39m# predict NER tags\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tagger\u001b[39m.\u001b[39mpredict(sentence)\n\u001b[1;32m      6\u001b[0m \u001b[39m# print sentence\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(sentence)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tagger' is not defined"
     ]
    }
   ],
   "source": [
    "sentence = Sentence(\"Apple sleept Samsung voor de rechter vanwege schending van patenten.\")\n",
    "\n",
    "# predict NER tags\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print sentence\n",
    "print(sentence)\n",
    "\n",
    "# print predicted NER spans\n",
    "print('The following NER tags are found:')\n",
    "# iterate over entities and print\n",
    "for entity in sentence.get_spans('ner'):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
