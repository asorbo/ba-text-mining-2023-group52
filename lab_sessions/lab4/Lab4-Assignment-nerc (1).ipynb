{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4-Assignment about Named Entity Recognition and Classification\n",
    "\n",
    "This notebook describes the assignment of Lab 4 of the text mining course. We assume you have succesfully completed Lab1, Lab2 and Lab3 as welll. Especially Lab2 is important for completing this assignment.\n",
    "\n",
    "**Learning goals**\n",
    "* going from linguistic input format to representing it in a feature space\n",
    "* working with pretrained word embeddings\n",
    "* train a supervised classifier (SVM)\n",
    "* evaluate a supervised classifier (SVM)\n",
    "* learn how to interpret the system output and the evaluation results\n",
    "* be able to propose future improvements based on the observed results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This notebook was originally created by [Marten Postma](https://martenpostma.github.io) and [Filip Ilievski](http://ilievski.nl) and adapted by Piek vossen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 18] Exercise 1 (NERC): Training and evaluating an SVM using CoNLL-2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 point] a) Load the CoNLL-2003 training data using the *ConllCorpusReader* and create for both *train.txt* and *test.txt*:**\n",
    "\n",
    "    [2 points]  -a list of dictionaries representing the features for each training instances, e..g,\n",
    "    ```\n",
    "    [\n",
    "    {'words': 'EU', 'pos': 'NNP'}, \n",
    "    {'words': 'rejects', 'pos': 'VBZ'},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    [2 points] -the NERC labels associated with each training instance, e.g.,\n",
    "    dictionaries, e.g.,\n",
    "    ```\n",
    "    [\n",
    "    'B-ORG', \n",
    "    'O',\n",
    "    ....\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 training features:  [{'words': 'EU', 'pos': 'NNP'}, {'words': 'rejects', 'pos': 'VBZ'}, {'words': 'German', 'pos': 'JJ'}, {'words': 'call', 'pos': 'NN'}, {'words': 'to', 'pos': 'TO'}, {'words': 'boycott', 'pos': 'VB'}, {'words': 'British', 'pos': 'JJ'}, {'words': 'lamb', 'pos': 'NN'}, {'words': '.', 'pos': '.'}, {'words': 'Peter', 'pos': 'NNP'}]\n",
      "First 10 gold labels:  ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-PER']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "#train = ConllCorpusReader('/Users/selmadissing/Documents/2. BSc Artificial Intelligence/Year 3/P4/Text Mining/ba-text-mining-2023-group52/lab_sessions/lab4/CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "train = ConllCorpusReader('/Users/snake/TextMining/Lab4/CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "training_features = []\n",
    "training_gold_labels = []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    a_dict = {'words': token, 'pos': pos}\n",
    "    training_features.append(a_dict)\n",
    "    training_gold_labels.append(ne_label)\n",
    "print(\"First 10 training features: \", training_features[:10])\n",
    "print(\"First 10 gold labels: \", training_gold_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 test features:  [{'words': 'SOCCER', 'pos': 'NN'}, {'words': '-', 'pos': ':'}, {'words': 'JAPAN', 'pos': 'NNP'}, {'words': 'GET', 'pos': 'VB'}, {'words': 'LUCKY', 'pos': 'NNP'}, {'words': 'WIN', 'pos': 'NNP'}, {'words': ',', 'pos': ','}, {'words': 'CHINA', 'pos': 'NNP'}, {'words': 'IN', 'pos': 'IN'}, {'words': 'SURPRISE', 'pos': 'DT'}]\n",
      "First 10 gold labels:  ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "#test = ConllCorpusReader('/Users/selmadissing/Documents/2. BSc Artificial Intelligence/Year 3/P4/Text Mining/ba-text-mining-2023-group52/lab_sessions/lab4/CONLL2003', 'test.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "test = ConllCorpusReader('/Users/snake/TextMining/Lab4/CONLL2003', 'test.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "test_features = []\n",
    "test_gold_labels = []\n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    a_dict = {'words': token, 'pos': pos}\n",
    "    test_features.append(a_dict)\n",
    "    test_gold_labels.append(ne_label)\n",
    "print(\"First 10 test features: \", test_features[:10])\n",
    "print(\"First 10 gold labels: \", test_gold_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] b) provide descriptive statistics about the training and test data:**\n",
    "* How many instances are in train and test?\n",
    "* Provide a frequency distribution of the NERC labels, i.e., how many times does each NERC label occur?\n",
    "* Discuss to what extent the training and test data is balanced (equal amount of instances for each NERC label) and to what extent the training and test data differ?\n",
    "\n",
    "Tip: you can use the following `Counter` functionality to generate frequency list of a list:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many instances are in train and test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances in train data: 203621\n",
      "Instances in train data: 46435\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print('Instances in train data:', len(training_features))\n",
    "print('Instances in train data:', len(test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide a frequency distribution of the NERC labels, i.e., how many times does each NERC label occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train NERC label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O</td>\n",
       "      <td>169578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>4528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I-ORG</td>\n",
       "      <td>3704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I-MISC</td>\n",
       "      <td>1155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I-LOC</td>\n",
       "      <td>1157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-PER</td>\n",
       "      <td>6600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>6321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-MISC</td>\n",
       "      <td>3438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>7140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Train NERC label   count\n",
       "1                O  169578\n",
       "4            I-PER    4528\n",
       "6            I-ORG    3704\n",
       "7           I-MISC    1155\n",
       "8            I-LOC    1157\n",
       "3            B-PER    6600\n",
       "0            B-ORG    6321\n",
       "2           B-MISC    3438\n",
       "5            B-LOC    7140"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_gold_freq = Counter(training_gold_labels)\n",
    "\n",
    "df_training = pd.DataFrame.from_dict(training_gold_freq, orient='index').reset_index()\n",
    "df_training = df_training.rename(columns={'index':'Train NERC label', 0:'count'})\n",
    "df_training = df_training.sort_values(by=['Train NERC label'], ascending=False)\n",
    "\n",
    "df_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test NERC label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>38323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>1156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I-ORG</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I-MISC</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I-LOC</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-PER</td>\n",
       "      <td>1617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>1661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B-MISC</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>1668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Test NERC label  count\n",
       "0               O  38323\n",
       "3           I-PER   1156\n",
       "8           I-ORG    835\n",
       "6          I-MISC    216\n",
       "4           I-LOC    257\n",
       "2           B-PER   1617\n",
       "7           B-ORG   1661\n",
       "5          B-MISC    702\n",
       "1           B-LOC   1668"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_gold_freq = Counter(test_gold_labels)\n",
    "\n",
    "df_test = pd.DataFrame.from_dict(test_gold_freq, orient='index').reset_index()\n",
    "df_test = df_test.rename(columns={'index':'Test NERC label', 0:'count'})\n",
    "df_test = df_test.sort_values(by=['Test NERC label'], ascending=False)\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss to what extent the training and test data is balanced (equal amount of instances for each NERC label) and to what extent the training and test data differ?\n",
    "\n",
    "The data appears to be somewhat balanced, with both the training and testing data showing the highest frequency of the O NERC label, as well as the lowest frequency being I-MISC label. Furthermore, the actual order of data labels grouped by label from least to most is the same for both sets. The training and test data slightly differ in the frequency of I-PER labeled data (2.22% vs 2.49%) and B-ORG labeled data (3.1% vs 3.58%), however most of the distribution of the data is quite similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] c) Concatenate the train and test features (the list of dictionaries) into one list. Load it using the *DictVectorizer*. Afterwards, split it back to training and test.**\n",
    "\n",
    "Tip: You’ve concatenated train and test into one list and then you’ve applied the DictVectorizer.\n",
    "The order of the rows is maintained. You can hence use an index (number of training instances) to split the_array back into train and test. Do NOT use: `\n",
    "from sklearn.model_selection import train_test_split` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "vec = DictVectorizer()\n",
    "all_feats = training_features + test_features\n",
    "the_array = vec.fit_transform(all_feats).toarray()\n",
    "train_split = the_array[:203621]\n",
    "test_split = the_array[203621:]\n",
    "print(train_split)\n",
    "print(test_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] d) Train the SVM using the train features and labels and evaluate on the test data. Provide a classification report (sklearn.metrics.classification_report).**\n",
    "The train (*lin_clf.fit*) might take a while. On my computer, it took 1min 53s, which is acceptable. Training models normally takes much longer. If it takes more than 5 minutes, you can use a subset for training. Describe the results:\n",
    "* Which NERC labels does the classifier perform well on? Why do you think this is the case?\n",
    "* Which NERC labels does the classifier perform poorly on? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### [ YOUR CODE SHOULD GO HERE ]\n",
    "# lin_clf.fit( # your code here\n",
    "lin_clf.fit(train_split, training_gold_labels)\n",
    "predict_label = lin_clf.predict(test_split)\n",
    "print(classification_report(test_gold_labels, predict_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6 points] e) Train a model that uses the embeddings of these words as inputs. Test again on the same data as in 2d. Generate a classification report and compare the results with the classifier you built in 2d.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('/Users/snake/TextMining/data/GoogleNews-vectors-negative300.bin.gz', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_train=[]\n",
    "embedding_gold_labels=[]\n",
    "embedding_test=[]\n",
    "embedding_gold_test_labels=[]\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    \n",
    "    if token!='' and token!='DOCSTART':\n",
    "        if token in word_embedding_model:\n",
    "            vector=word_embedding_model[token]\n",
    "        else:\n",
    "            vector=[0]*300\n",
    "        embedding_train.append(vector)\n",
    "        embedding_gold_labels.append(ne_label)\n",
    "        \n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    \n",
    "    if token!='' and token!='DOCSTART':\n",
    "        if token in word_embedding_model:\n",
    "            vector=word_embedding_model[token]\n",
    "        else:\n",
    "            vector=[0]*300\n",
    "        embedding_test.append(vector)\n",
    "        embedding_gold_test_labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.76      0.80      0.78      1668\n",
      "      B-MISC       0.72      0.70      0.71       702\n",
      "       B-ORG       0.69      0.64      0.66      1661\n",
      "       B-PER       0.75      0.67      0.71      1617\n",
      "       I-LOC       0.51      0.42      0.46       257\n",
      "      I-MISC       0.60      0.54      0.57       216\n",
      "       I-ORG       0.48      0.33      0.39       835\n",
      "       I-PER       0.59      0.50      0.54      1156\n",
      "           O       0.97      0.99      0.98     38323\n",
      "\n",
      "    accuracy                           0.93     46435\n",
      "   macro avg       0.68      0.62      0.64     46435\n",
      "weighted avg       0.92      0.93      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lin_clf.fit(embedding_train, embedding_gold_labels)\n",
    "predict_label = lin_clf.predict(embedding_test)\n",
    "print(classification_report(embedding_gold_test_labels, predict_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision seems to be lower when using embeddings as input; this being said, the difference does not seem to be sizable, and the accuracy, macro average, and weighted average still appear to do well. One exception is the inside person tag, which shows a greater improvement when using embeddings of the words as an input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 10] Exercise 2 (NERC): feature inspection using the [Annotated Corpus for Named Entity Recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)\n",
    "**[6 points] a. Perform the same steps as in the previous exercise. Make sure you end up for both the training part (*df_train*) and the test part (*df_test*) with:**\n",
    "* the features representation using **DictVectorizer**\n",
    "* the NERC labels in a list\n",
    "\n",
    "Please note that this is the same setup as in the previous exercise:\n",
    "* load both train and test using:\n",
    "    * list of dictionaries for features\n",
    "    * list of NERC labels\n",
    "* combine train and test features in a list and represent them using one hot encoding\n",
    "* train using the training features and NERC labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bp/dskwzdl538z3rk5w7cvf6bwc0000gp/T/ipykernel_9968/937407749.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  kaggle_dataset = pandas.read_csv(path, error_bad_lines=False)\n",
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    }
   ],
   "source": [
    "##### Adapt the path to point to your local copy of NERC_datasets\n",
    "path = '/Users/snake/TextMining/Lab4/kaggle/ner_v2.csv'\n",
    "kaggle_dataset = pandas.read_csv(path, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050795"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kaggle_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 20000\n"
     ]
    }
   ],
   "source": [
    "df_train = kaggle_dataset[:100000]\n",
    "df_test = kaggle_dataset[100000:120000]\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "\n",
    "training_features2 = []\n",
    "training_gold_labels2 = []\n",
    "test_features2 = []\n",
    "test_gold_labels2 = []\n",
    "for index, instance in df_train.iterrows():\n",
    "    a_dict = {\n",
    "        'id': index,\n",
    "        'lemma':instance['lemma'],\n",
    "        'pos':instance['pos'],\n",
    "        'shape':instance['shape'],\n",
    "        'word':instance['word'],\n",
    "        'next-lemma': instance['next-lemma'],\n",
    "        'next-pos': instance['next-pos'],\n",
    "        'next-shape': instance['next-shape'],\n",
    "        'next-word': instance['next-word'],\n",
    "        'next-next-lemma': instance['next-next-lemma'],\n",
    "        'next-next-pos': instance['next-next-pos'],\n",
    "        'next-next-shape': instance['next-next-shape'],\n",
    "        'next-next-word': instance['next-next-word'],\n",
    "        'prev-iob':instance['prev-iob'],\n",
    "        'prev-lemma':instance['prev-lemma'],\n",
    "        'prev-pos':instance['prev-pos'],\n",
    "        'prev-shape':instance['prev-shape'],\n",
    "        'prev-word':instance['prev-word'],\n",
    "        'prev-prev-iob':instance['prev-prev-iob'],\n",
    "        'prev-prev-lemma':instance['prev-prev-lemma'],\n",
    "        'prev-prev-pos':instance['prev-prev-pos'],\n",
    "        'prev-prev-shape':instance['prev-prev-shape'],\n",
    "        'prev-prev-word':instance['prev-prev-word'],\n",
    "        'sentence_idx':instance['sentence_idx']   \n",
    "    }\n",
    "    training_features2.append(a_dict)\n",
    "    training_gold_labels2.append(instance['tag'])\n",
    "    \n",
    "for index, instance in df_test.iterrows():\n",
    "    a_dict = {\n",
    "        'id': index,\n",
    "        'lemma':instance['lemma'],\n",
    "        'pos':instance['pos'],\n",
    "        'shape':instance['shape'],\n",
    "        'word':instance['word'],\n",
    "        'next-lemma': instance['next-lemma'],\n",
    "        'next-pos': instance['next-pos'],\n",
    "        'next-shape': instance['next-shape'],\n",
    "        'next-word': instance['next-word'],\n",
    "        'next-next-lemma': instance['next-next-lemma'],\n",
    "        'next-next-pos': instance['next-next-pos'],\n",
    "        'next-next-shape': instance['next-next-shape'],\n",
    "        'next-next-word': instance['next-next-word'],\n",
    "        'prev-iob':instance['prev-iob'],\n",
    "        'prev-lemma':instance['prev-lemma'],\n",
    "        'prev-pos':instance['prev-pos'],\n",
    "        'prev-shape':instance['prev-shape'],\n",
    "        'prev-word':instance['prev-word'],\n",
    "        'prev-prev-iob':instance['prev-prev-iob'],\n",
    "        'prev-prev-lemma':instance['prev-prev-lemma'],\n",
    "        'prev-prev-pos':instance['prev-prev-pos'],\n",
    "        'prev-prev-shape':instance['prev-prev-shape'],\n",
    "        'prev-prev-word':instance['prev-prev-word'],\n",
    "        'sentence_idx':instance['sentence_idx']   \n",
    "    }\n",
    "    test_features2.append(a_dict)\n",
    "    test_gold_labels2.append(instance['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = vec.fit_transform(training_features2[:]).toarray()\n",
    "test2 = vec.fit_transform(test_features2).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 combined train and test features: \n",
      " [[0.00000e+00 0.00000e+00 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [1.00000e+00 0.00000e+00 0.00000e+00 ... 0.00000e+00 1.00000e+00\n",
      "  0.00000e+00]\n",
      " [2.00000e+00 0.00000e+00 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " ...\n",
      " [1.00002e+05 0.00000e+00 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00]\n",
      " [1.00003e+05 0.00000e+00 1.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [1.00004e+05 1.00000e+00 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# combine train and test features in a list and represent them using one hot encoding\n",
    "all_features = training_features2[:5] + test_features2[:5]\n",
    "combined_features = vec.fit_transform(all_features).toarray()\n",
    "print(\"The first 5 combined train and test features: \\n\", combined_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] b. Train and evaluate the model and provide the classification report:**\n",
    "* use the SVM to predict NERC labels on the test data\n",
    "* evaluate the performance of the SVM on the test data\n",
    "\n",
    "Analyze the performance per NERC label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
